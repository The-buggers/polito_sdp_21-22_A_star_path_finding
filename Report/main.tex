%% arara directives
% arara: xelatex
% arara: bibtex
% arara: xelatex
% arara: xelatex

%\documentclass{article} % One-column default
\documentclass[twocolumn, switch]{article} % Method A for two-column formatting

\usepackage{preprint}
\newcommand{\removelatexerror}{\let\@latex@error\@gobble}
%% Math packages
\usepackage{amsmath, comment, amsthm, amssymb, amsfonts, algorithm2e, algpseudocode, multicol,  graphicx, caption, subcaption, multirow}
\RestyleAlgo{ruled}
%\SetAlFnt{\scriptsize}
\graphicspath{ {./Pictures/} }
%% Bibliography options
\usepackage[numbers,square]{natbib}
\bibliographystyle{unsrtnat}
%\usepackage{natbib}
%\bibliographystyle{Geology}

%% General packages
\usepackage[utf8]{inputenc}	% allow utf-8 input
\usepackage[T1]{fontenc}	% use 8-bit T1 fonts
\usepackage{xcolor}		% colors for hyperlinks
\usepackage[colorlinks = true,
            linkcolor = purple,
            urlcolor  = blue,
            citecolor = cyan,
            anchorcolor = black]{hyperref}	% Color links to references, figures, etc.
\usepackage{booktabs} 		% professional-quality tables
\usepackage{nicefrac}		% compact symbols for 1/2, etc.
\usepackage{microtype}		% microtypography
\usepackage{lineno}		% Line numbers
\usepackage{float}			% Allows for figures within multicol
%\usepackage{multicol}		% Multiple columns (Method B)

\usepackage{lipsum}		%  Filler text

 %% Special figure caption options
\usepackage{newfloat}
\DeclareFloatingEnvironment[name={Supplementary Figure}]{suppfigure}
\usepackage{sidecap}
\sidecaptionvpos{figure}{c}

% Section title spacing  options
\usepackage{titlesec}
\titlespacing\section{0pt}{12pt plus 3pt minus 3pt}{1pt plus 1pt minus 1pt}
\titlespacing\subsection{0pt}{10pt plus 3pt minus 3pt}{1pt plus 1pt minus 1pt}
\titlespacing\subsubsection{0pt}{8pt plus 3pt minus 3pt}{1pt plus 1pt minus 1pt}

% ORCiD insertion
\usepackage{tikz,xcolor,hyperref}

\definecolor{lime}{HTML}{A6CE39}
\DeclareRobustCommand{\orcidicon}{
	\begin{tikzpicture}
	\draw[lime, fill=lime] (0,0) 
	circle [radius=0.16] 
	node[white] {{\fontfamily{qag}\selectfont \tiny ID}};
	\draw[white, fill=white] (-0.0625,0.095) 
	circle [radius=0.007];
	\end{tikzpicture}
	\hspace{-2mm}
}
\foreach \x in {A, ..., Z}{\expandafter\xdef\csname orcid\x\endcsname{\noexpand\href{https://orcid.org/\csname orcidauthor\x\endcsname}
			{\noexpand\orcidicon}}
}
% Define the ORCID iD command for each author separately. Here done for two authors.
\newcommand{\orcidauthorA}{0000-0000-0000-0001}
\newcommand{\orcidauthorB}{0000-0000-0000-0002}
\newcommand{\orcidauthorC}{0000-0000-0000-0003}
\newcommand{\orcidauthorD}{0000-0000-0000-0004}

%%%%%%%%%%%%%%%%   Title   %%%%%%%%%%%%%%%%
\title{Parallel A* project}

% Add watermark with submission status
\usepackage{xwatermark}
% Left watermark
\newwatermark[firstpage,color=gray!60,angle=90,scale=0.32, xpos=-4.05in,ypos=0]{\href{https://doi.org/}{\color{gray}{Publication doi}}}
% Right watermark
\newwatermark[firstpage,color=gray!60,angle=90,scale=0.32, xpos=3.9in,ypos=0]{\href{https://doi.org/}{\color{gray}{Preprint doi}}}
% Bottom watermark
\newwatermark[firstpage,color=gray!90,angle=0,scale=0.28, xpos=0in,ypos=-5in]{*correspondence: \texttt{email@institution.edu}}

%%%%%%%%%%%%%%%  Author list  %%%%%%%%%%%%%%%
\usepackage{authblk}
\renewcommand*{\Authfont}{\bfseries}
\author{Lorenzo Ippolito, Mattia Rosso, Fabio Mirto}


%%%%%%%%%%%%%%    Front matter    %%%%%%%%%%%%%%
\begin{document}

\twocolumn[ % Method A for two-column formatting
  \begin{@twocolumnfalse} % Method A for two-column formatting
  
\maketitle

\begin{abstract}
  ...Here the abstract...
\end{abstract}
%\keywords{First keyword \and Second keyword \and More} % (optional)
\vspace{0.35cm}

  \end{@twocolumnfalse} % Method A for two-column formatting
] % Method A for two-column formatting

%\begin{multicols}{2} % Method B for two-column formatting (doesn't play well with line numbers), comment out if using method A


%%%%%%%%%%%%%%%  Main text   %%%%%%%%%%%%%%%
% \linenumbers

%% %%%%%%%%%%%%%%%%%%%%%%% %%
%% CHAPTER 1: INTRODUCTION %%
%% %%%%%%%%%%%%%%%%%%%%%%% %%
\section{Introduction: about the A* algorithm}
A* is a graph-traversal and path-search algorithm. It is used in many contexts of computer science and 
not only. It can be considered as a general case of the Dijkstra algorithm. It is a Greedy-best-first-search algorithm that uses an heuristic function to guide
itself. What it does is combining:
\begin{itemize}
  \item Dijkstra approach: favoring nodes closed to the starting point(source)
  \item Greedy-best-first-search approach: favoring nodes closed to the final point(destination)
\end{itemize}
Using the standard terminology we have:
\begin{itemize}
  \item $g(n)$: exact cost of moving from source to $n$.
  \item $h(n)$: heuristic estimated cost of moving from a node $n$ to the destination.
  \item $f(n) = g(n) + h(n)$: combination of the actual cost with the estimated one.
\end{itemize}
At each iteration the node $n$ that has the minimum $f(n)$ is examinated(expanded).
\subsection{Heuristic design}
\paragraph{Premises} We are going to work with weighted oriented graphs where $V$ is the set on nodes/vertices
and $E$ is the set of edges of the form $(x,y)$ to indicate that an oriented edge from $x$ to $y$ exists and it
has weight indicated as $cost(x, y)$.
\paragraph{Heuristic properties} 
The heurstic function represents the acutal core of the A* algorithm. It represents a prior-knowledge that
we have about the cost of the path from every node (source included) to the destination. 
\begin{itemize}
  \item If we have not this prior information($h(n) = 0 \;\forall n \in V$) we are turning the A* algorithm into
        Dijkstra (this is why A* can be considered as a more general case of Dijkstra algorithm) but we always
        have the guarantee of finding the shortest path. 
  \item \textit{Admissible} Heuristic: if $h(n) < cost(n, dest) \;\forall n \in V$ (so the we never over-estimate the distance to
        get to the destination from a node $n$) A* will always find the shortest path and the heuristic
        function is called \textit{admissible}. The more inaccurate
        is the estimation the more nodes A* will need to expand (with the upper bound of expanded nodes
        if $h(n) = 0$).
  \item \textit{Consistent} Heuristic: if $h(x) \le cost(x, y) + h(y)$ for every edge $(x, y)$ (so the triangular
        inequality is satisfied) A* has the guarantee to find an optimal path without processing
        any node more than once. 
\end{itemize}
\paragraph{Corner cases} 
\begin{itemize}
  \item Dijkstra: As already discussed if $h(n)=0$ for every node in the graph A* turns into the Dijkstra algorithm.
  \item Ideal: We would obtain a perferct behaviour in case $h(n)$ is exactly equal to the cost of moving from $n$ to
  the destination (A* will only expand the nodes on the best path to get to the destination).
  \item Full greedy-best-first search: if $h(n) \gg g(n)$ than only $h(n)$ plays a role and A* turns into
        a completly greedy-best-first search algorithm.
\end{itemize}

%% %%%%%%%%%%%%%%%%%%%%%% %%
%% CHAPTER 2: APPLICATION %%
%% %%%%%%%%%%%%%%%%%%%%%% %%
\section{A* project application}
\paragraph{Problem definition} Given a wide range of fields where the A* algorithm can be applied we have choosed the one of
optimal path searching in geographical areas where the goal is to find the minimum distance path from a 
node source to a node destination.

\paragraph{Notation} We work with a weighted oriented graph $G$ that is made of nodes $n \in V$ that represents
road-realated points of interest and edges $(x,y) \in E$ represent the unidirectional connections among these points.
Each edge $(x, y)$ is associated to a weight that is the great-circle distance between $x$ and $y$ measured
in meters.
\paragraph{Benchmark} We will exploit the DIMACS benchmark to make robust estimates of the designed algortithms.
Starting from the FIPS system format files provided we have adapted them (as better explained in section 3) to provide
to the algorithms a file containing information structured as:
\begin{itemize}
  \item Nodes: each node $n$ is defined as $(index, longitude, latitude)$ where $index$ is a natural progressive
        number starting from $0$ used to univocally identify the node and $(longitude, latitude)$ are the geographical
        coordinates of the node.
  \item Edges: each edge $(x, y)$ is defined as $(x, y, weight)$ and it represents a unidirectional connection
        from $x$ to $y$ (a road) with length $weight$ (great-circle distance from $x$ to $y$).
\end{itemize}
\subsection{Heuristic function: the great-circle distance}
As previoulsly discussed the A* algorithm needs an admssible and consistent heuristic to properly work and this
function is tipically problem-specific. Given the type of problem we are going to apply A* to we are going
to use a measure of geographical distance that extends the concept of heuclidean distance between two points:
the great circle distance (that is the shortest distance over the earth surface measured along the
earth surface itself).
\begin{figure}[ht!]
  \centering
  \includegraphics[width=0.5\linewidth]{haversine.png}
  \caption{Great circle distance from P to Q}
  \label{haversine}
\end{figure}
We will employee the \textbf{Haversine formula} to compute the distance from node $(\phi_1,\lambda_1)$
and node $(\phi_1,\lambda_1)$ where $\phi$ is the latitude and $\lambda$ is the longitude:
\begin{center}
  $d = R \cdot c$\\
  where $c = 2 \cdot atan2(\sqrt{a},\sqrt{1-a})$\\
  $a = sin^2\Big({\frac{\Delta \phi}{2}}\Big) + cos(\phi_1) \cdot cos(\phi_2) \cdot sin^2\Big({\frac{\Delta \lambda}{2}}\Big)$
  \\$R$ is the earth radius that we have fixed to $R=6.371km$
\end{center}

%% %%%%%%%%%%%%%%%%%%%%%% %%
%% CHAPTER 1: INPUT GRAPH %%
%% %%%%%%%%%%%%%%%%%%%%%% %%
\section{Graph file input}
\paragraph{File input format} 
Now we start discussing the A* algorithm implementation and to do that we need to specify which 
types of files we will need to provide to the algorithm to load the graph of interest. Each file has 
the format:
\begin{itemize}
    \item First line: the number of nodes $N[int]$
    \item N following lines: nodes appearing as $(index[int], longitude[double], latidue[double])$
    \item E following lines (with E unknown): edges appearing as $(x[int], y[int], weight[double])$
\end{itemize}
\paragraph{Random test graph} We have tested the designed algorithms also on a random generated graph that
is built starting from these information: 
\begin{itemize}
  \item Which path we want to find: given the couple (source, destination) it is generated a graph of
        $max(source, destination) + 1$ nodes.
  \item How many paths at most have to be generated from source to destination.
  \item The maximum length of these paths (that will be randomly chosen for each path).
\end{itemize}
In this way we have ad-hoc files to stress the algorithm having the guarantee that more than one path
exists from source to destination (other more standart approaches exist for random graph generation but we
have implemented this custom one for this reason). To be consistent with benchmark files also these
random graphs represent geographic points identified by longitude and latitude.
\paragraph{DIMACS benchmark} 
The benchmark files we have used come from the DIMACS benchmark. Here each geographic map is described by:
\begin{itemize}
  \item \textit{.co} file: a file containing the coordinates of the nodes following the FIPS system notation.
  \item \textit{.gr} file: a file containing the edges and the relative weight(distance) expressed in meters.
\end{itemize}
The generation of a file consistent with the format described above happens by merging these files 
into a new one (in binary format). One of the challenge we are going to undertake is the one of
parallelizing the reading of these huge files (that we will show having an high impact in terms of
execution time over the overall A* algorithm).
\paragraph{Test paths}
To analyze the performance of the different versions of A* algorithms we have used these paths:
\begin{table}[ht!]
	\caption{Test paths for A*}
	\centering
	\begin{tabular}{ |l|l|l|l| }
		\hline
		\textit{Nodes} & \textit{Edges} & \textit{Source} & \textit{Dest} \\ 
    \hline
		\multicolumn{4}{ |c| }{\bf{Random map}} \\
		\hline
		\multirow{1}{*}{}
		 101& - & 0 & 100\\
		\hline
		\multicolumn{4}{ |c| }{\bf{California(BAY)}} \\
		\hline
		\multirow{1}{*}{}
		321270& 800172 & 321269 & 263446\\
		\hline
		\multicolumn{4}{ |c| }{\bf{Florida(FLA)}} \\
		\hline
		\multirow{1}{*}{}
		1070376 & 2712798 & 0 & 103585\\
		\hline
    \multicolumn{4}{ |c| }{\bf{Western USA(W)}} \\
		\hline
		\multirow{1}{*}{}
		6262104 & 15248146 & 1523755 & 1953083\\
		\hline
    \multicolumn{4}{ |c| }{\bf{Full USA(USA)}} \\
		\hline
		\multirow{1}{*}{}
		23947347 & 58333344 & 14130775 & 810300\\
		\hline
	\end{tabular}
\end{table}
\begin{figure}[ht!]
  \centering
  \includegraphics[width=0.8\linewidth]{google_maps.png}
  \caption{From left to right top to bottom BAY, FLA, W, USA}
  \label{testpaths}
\end{figure}

%% %%%%%%%%%%%%%%%%%%%%%%%% %%
%% CHAPTER 1: A* SEQUENTIAL %%
%% %%%%%%%%%%%%%%%%%%%%%%%% %%
\section{Sequential A* Algorithm}
Sequential A* algorithm is the one we will start with to see how the algorithm works and performs.
The first step consists of a pre-computation of:
\begin{itemize}
  \item The heurstic $h(n)$ for each node (by defintion the $h(dest)$ will
        be 0) computed through the Haversine formula. We thus keep a global data
        structure \texttt{h} to do this.
  \item The initial values of $f(n)$ and $g(n)$ that will be set to $DOUBLE\_MAX$ for each
        node except for the source node that will have $f(source) = h(source)$ because 
        $g(soruce)$ is clearly $0$. We thus keep global data structures \texttt{f} and \texttt{g} to do this.
\end{itemize}
We will also need two additional data structures:
\begin{itemize}
  \item The \textit{costToCome} table (where \texttt{costToCome[i]} contains the current
        best cost to reach node \texttt{i}) that is initialized
        to $DOUBLE\_MAX$.
  \item The \textit{parentVertex} table (where \texttt{parentVertex[i]} contains the
        parent node of node \texttt{i} according to the current best path found to reach
        the destination) that is initialized with $-1$.
\end{itemize}
The outer loop is based on the nodes extraction from a \textit{open set}, the one containing
the nodes that have still to be explored (it is implemented as a Priory Queue where the priority
is associated to the $f(n)$ of the nodes). At each iteration the node $a$ with minimum $f(a)$ is
extracted from the \textit{open set} and its neighbors are expanded: the inner loop is repeated once for each neighbor $b$ of the
extracted node $a$($b$ is neighbor of $a$ if the edge $(a, b)$ exists). A tentative score is computed
for each node $b$ as $g(b) = g(a) + weight(a, b)$. If $g(b)$ is less than current \texttt{g[b]} these data structures
are updated: 
\begin{flushleft}
  \texttt{g[b] = g[a] + weight(a, b)}\\
  \texttt{costToCome[b] = g[b]}\\
  \texttt{parentVertex[b] = a}\\
  \texttt{f[b] = g[b] + h[b]}
\end{flushleft}
The final step of this inner loop is checking wheter the node $b$ has already been added to the 
\textit{open set} or not. In case the \textit{open set} doesn't contain it we add it (with priority
$f(n)$ just computed).
Since the heuristic function we have chosen is both admissible and consistent we have the guarantee 
that the first time a node is extracted from the \textit{open set} we have found a best path to it (this
is why when the destination node is extracted we can terminate having found the best path). So despite
the node may be added to the \textit{open set} more than once (when discovered as a neighbor of different nodes)
it will be only expanded once.
\SetKwComment{Comment}{/* }{ */}
\begin{algorithm}[ht!] 
\caption{Sequential A*}\label{alg:two}
\KwData{Graph G(V,E), Source s, Destination d, Heurstic h}
\KwResult{Best path from Source to Destination and relative cost}
$g[i] \gets DOUBLE\_MAX \;\forall i \in V$\;
$f[i] \gets DOUBLE\_MAX \;\forall i \in V$\;
$h[i] \gets h(i, d) \; \forall i \in V$\;
$costToCome[i] \gets 1 \; \forall i \in V$\;
$parentVertex[i] \gets 1 \; \forall i \in V$\;
$f[s] \gets h[s]$\;
$g[s] \gets 0$\;
$openSet := \{(s, f[s])\}$\;
\While{$!openSet.EMPTY()$}{
  $a \gets openSet.POP()$\;
  \If{$a == d$}{
    $pathFound \gets true$\;
    reconstructPath()\;
  }
  %\Comment*[r]{Inner loop}
  \ForEach{neighbor $b$ of $a$}{
    $wt \gets weight(a, b)$\;
    $tentativeScore \gets g[a] + wt$\;
    \If{$tentativeScore$ is less than $g[b]$}{
      $parentVertex[b] \gets a$\;
      $costToCome[b] \gets wt$\;
      $g[b] \gets tentativeScore$\;
      $f[b] \gets g[b] + h[b]$\;
      $openSet.PUSH((b, f[b]))$\;
    }
  }
}
\end{algorithm}

\subsection{Results}
We are now going to run the sequential A* algorithm.
\begin{table}[]
  \caption{Sequential algorithms performance}
  \label{seqtimes}
  \begin{tabular}{|l|l|l|l|l|l|}
  \hline
  \textbf{}    & \textbf{File Size} & \textbf{Reading} & \textbf{A*} & \textbf{Total} & \textbf{\begin{tabular}[c]{@{}l@{}}Reading \\ Impact\end{tabular}} \\ \hline
  \textbf{RND} & 2876B                & 0.0011s           & 0.0519s      & 0.0530s          & 2.1\%                                                           \\ \hline
  \textbf{BAY} & 20.51MB              & 0.9538s           & 0.2197s      & 1.1735s          & 81.3\%                                                         \\ \hline
  \textbf{FLA} & 69.09MB              & 3.1551s           & 0.7174s      & 3.8725s          & 81.5\%                                                           \\ \hline
  \textbf{W}   & 394.26MB             & 18.3065s          & 2.5890s      & 20.8955s         & 87.6\%                                                           \\ \hline
  \textbf{USA} & 1292.40MB            & 56.9942s          & 13.6716s     & 70.6658s         & 80.6\%                                                           \\ \hline
  \end{tabular}
\end{table}
We can realize that from table \ref{seqtimes} (despite for the random graph that is too small to appreciate this result) the 
reading time has a very high impact on the overall execution time of the algorithm and in section 6
we will investigate three different techniques for parallelizing the reading of the file. 
% %%%%%%%%%%%%%%%%%%%%%%%%% %
% CHAPTER 5: A* vs DIJKSTRA %
% %%%%%%%%%%%%%%%%%%%%%%%%% %
\section{A* and Dijkstra: a comparison}
As already mentioned the Dijkstra algorithm can be considered as a particular case of A* where we
don't have any prior knowledge about the distances beteween the nodes ($h(n) = 0 \;\forall n \in V$).
We have also seen that the more precise is the heuristic function we provide the less nodes the algorithm
will expand to get to the destination obtaining the best path. To investigate this point we have run
Dijkstra algorithm on the same graph comparing the number of expanded nodes by the two algorithms:

\begin{table}[ht!]
  \caption{Expanded nodes vs total nodes for Dijkstra and A*}
  \begin{tabular}{|l|l|l|}
  \hline
  \textbf{Expanded nodes} & \textbf{Dijkstra} & \textbf{Sequential A*}       \\ \hline
  \textbf{RND}            & 15 of 101           & 13 of 101         \\ \hline
  \textbf{BAY}            & 318725 of 321270    & 157137 of 321270  \\ \hline
  \textbf{FLA}            & 996956 of 1070376   & 592480 of 1070376 \\ \hline
  \textbf{W}              & 5470394 of 1070376  & 1600083 of 1070376 \\ \hline
  \textbf{USA}            & 16676528 of 1070376 & 8998767 of 1070376 \\ \hline
  \end{tabular}
\end{table}
\begin{figure}[ht!]
  \centering
  \includegraphics[width=1\linewidth]{astar_dijkstra/expanded_nodes.png}
  \caption{Expanded nodes: A* vs Dijkstra}
  \label{histogramnodes}
\end{figure}
The number of expanded nodes is clearly much higher when Dijkstra algorithm is used and the picture
\ref{astardijkstramap} cleary show in \textcolor{blue}{blue} the nodes expanded by the sequntial A* algorithm
while the \textcolor{red}{red} show the nodes expanded by the Dijkstra algorithm.
\begin{figure}[ht!]
  \centering
  \includegraphics[width=1\linewidth]{dijkstra_astar.png}
  \caption{Test paths on BAY(left) and FLA(right)}
  \label{astardijkstramap}
\end{figure}
In figure \ref{astardijkstratime} we can notice that the execution time is not necessary much better for A*.
This could depend both on the path we are looking for and on the precision of the heuristic function with
respect to the cost of the optimal path. From figure \ref{astardijkstracpumem} we realize that considering 
CPU and Memory usage A* needs more memory
because of the higher number of data structures allocated buy the less number of nodes expanded makes
the CPU usage inferior w.r.t Dijkstra algorithm.
\begin{figure}[ht!]
  \centering
  \includegraphics[width=1\linewidth]{astar_dijkstra/execution_time.png}
  \caption{Execution time: A* vs Dijkstra}
  \label{astardijkstratime}
\end{figure}
\begin{figure}[ht!]
  \centering
  \includegraphics[width=1\linewidth]{astar_dijkstra/cpumem.png}
  \caption{Exploited resources: A* vs Dijkstra}
  \label{astardijkstracpumem}
\end{figure}
\section{Parallel reading of the input file}
As we have seen in section 5 the time spent by the algorithm to read the input graph
(the input binary file) used by the A* algorithm is very high w.r.t. the total
amount of time spent. This is the reason why we have decided to inspect three techniques of
parallelization of the reading phase to speed it up. The input file, as already discussed,
is divided in two different sections (nodes and edges) so, in general, we need to take care of which
section a given thread is working on because different data structures of the graph need to
be loaded in the two sections (the symbol table when reading a \textit{node line} and the linked list when
reading a \textit{edge line}).
\subsection{Parallel Read: approach 1}
\begin{comment}
In this first approach we have implemeted a solution on which:
\begin{itemize}
  \item $N$ threads runs freely to read the entire file
  \item Only one file descriptor is shared among all the threads (this means that
        when thread $t_i$ performs a \texttt{read} opearation all the other threads 
        are waiting for it to finish)
\end{itemize}
This is the most trivial solution that can be adopted and the drawback is the really high
resource contention that exists among all the threads (when N threads whant to access line
$k$ of the file only $1$ can do it and others $N-1$ wait). The advantage is that the file
will be read \textit{sequentially} but in a multithread fashion (line $k$ of the file
is always read before line $k+1$) and this results in a easier implementation.
\paragraph{Results on FLA}
\begin{figure}[ht!]
  \centering
  \includegraphics[width=1\linewidth]{par_read_1_time.png}
  \caption{Performance of approach 1 for different number of threads}
  \label{parread1time}
\end{figure}
\end{comment}
\subsection{Parallel Read: approach 2}
(Explanation)
\paragraph{Results on FLA} 
As we can notice from figure \ref{parread2time}...
\begin{figure}[ht!]
  \centering
  \includegraphics[width=1\linewidth]{par_read_2_time.png}
  \caption{Performance of approach 2 for different number of threads on FLA}
  \label{parread2time}
\end{figure}
\subsection{Parallel Read: approach 3}
This is the approach is characterized by:
\begin{itemize}
  \item Letting threads differentiate among \textit{nodes section} and 
        \textit{edges section} to be able to read them togheter.
  \item Using a $(NP, NT)$ mechanism to read each section of the input file (where
        $NP$ is the number of partitions the section is divided in and $NT$ is the number
        of threads that have to read all the partitions).
\end{itemize}
\paragraph{The $(NP, NT)$ mechanism}
We need to provide as input:
\begin{itemize}
  \item $NP-nodes$: the number of partitions of the \textit{nodes section}
  \item $NP-edges$: the number of partitions of the \textit{edges section}
  \item $NT-nodes$: the number of threads that have to read the \textit{nodes section}
  \item $NT-edges$: the number of threads that have to read the \textit{edges section}
\end{itemize}
If the number of threads is equal to the number of partitions we have a simpler mechanism in
which each thread will read only one partition and then terminates.
As we can see in the example in figure \ref{parread3} threads will iterate over the partitions
of the sections that have been statically allocated to them. This means that there is not a real
contention of the reading phase since the partions are not overlapping and
each partition will be read by one and only one thread (actually, the
loading of the graph data structures after reading each line must be done in mutual exclusion so we will need a lock
both for the \textit{nodes-threads} and for the \textit{edges-threads}).
\begin{figure}[ht!]
  \centering
  \includegraphics[width=1\linewidth]{par_read_3.png}
  \caption{Example of parallel read - approach 3}
  \label{parread3}
\end{figure}
Each thread terminates when it has no more partitions to read.\\
\paragraph{Results on FLA}
\begin{figure}[ht!]
  \centering
  \includegraphics[width=1\linewidth]{par_read_3_time.png}
  \caption{Performance of approach 3 for different number of threads and partitions on FLA}
  \label{parread3time}
\end{figure}
We can realize from figure \ref{parread3time} that the number of partitions (setted equal
both for \textit{nodes section} and \textit{edges section}) has not a high impact.
What has more impact is the number of threads, an increasing number of threads gives increasing
bad performances while with a small number of threads we are able to obtain better results
w.r.t the first approach but worse result w.r.t the sequential reading. The reason of this
behaviour could be due to the fact that there is not a completly parallel work of the threads
since the access to Graph's data structure has to be done in mutual exclusion so we 
have benefits in dividing the threads in \textit{edges} threads and \textit{nodes} threads (something
that didn't happen in the first approach) but the mutual exclusion access to the Graph
makes the overhead due to threads creation and management not being able to obtain good
performances
\subsection{Final results}
\begin{figure}[ht!]
  \centering
  \includegraphics[width=1\linewidth]{par_read_all_time.png}
  \caption{RA(Read Approach) 1,2,3 compared with sequential reading on FLA}
  \label{parreadalltime}
\end{figure}
For the sake of completeness we have also test the most promising reading approach
on all the maps (BAY, FLA, W, USA) and results are showed in table \ref{readresultsfinal}
where the speed-up with respect to the sequential reading time is evident.
\begin{table}[ht!]
  \centering
    \caption{RA2 results against sequential reading}
    \begin{tabular}{|l|l|l|l|}
    \hline
    \textbf{}    & \textbf{\begin{tabular}[c]{@{}l@{}}RA2\\ (2 threads)\end{tabular}} & \textbf{Sequential} & \textbf{Speed-Up} \\ \hline
    \textbf{BAY} & 0.1936s                                                            & 0.9366s             & 79.3\%            \\ \hline
    \textbf{FLA} & 0.5103s                                                            & 3.0650s             & 83.4\%            \\ \hline
    \textbf{W} & 14.1492s                                                           & 56.4445s            & 74.9\%            \\ \hline
    \textbf{USA} & 14.1492s                                                           & 56.4445s            & 74.9\%            \\ \hline
    \end{tabular}
    \label{readresultsfinal}
\end{table}

\section{Parallel A*: two examinated approaches}
The goal of the project was to find one or more parallel versions of the A* algorithm 
and showing their performances w.r.t. the sequential version. We have choosen two approaches
to face the problem of parallelizing the A* algorithm: a first approach(known in literature as HDA*) that puts in action a
complex way of parallelizing the algorithm by defining a hash-based work distribution strategy and
a second approach that makes use of the parallelism in order to make the work of
finding the shortest path from \textit{source} to \textit{dest} split between only two threads
where one looks for the path from \textit{source} to \textit{dest} and the other one looks
for the path from \textit{dest} to \textit{source} in the reversed graph (the PNBA* algorithm).

\subsection{HDA*}
The Hash-Distributed-A* (HDA*) algorithm works in a completly different way w.r.t. the 
first attempt algorithm. It is based on the fact that each thread is \textit{owner} of a
specific set of nodes of the Graph: given a node $n$ it is defined a hash function $f:\;f(n) = t$
where $t \in \{1..N\}$ with $N$ the number of threads. When a thread extracts from the \textit{open set}
(expands) a node all its neighbors are added to the \textit{open set} of the owner thread of the expanded node. 
One important fact is that HDA* doesn't provide the same guarantees of the sequential algorithm:
\begin{itemize}
  \item In sequential A* if it's provided an heuristic function that is both \textit{admissible}
        and \textit{consistent} we have the guarantee that each node will be only expanded once
        and that the first time we expand that node we have found a shortest path to it.
  \item In HDA* we loose these guarantees: since we don't know in which order nodes will be processed
        it could happen that a longer path to \textit{dest} is found before the shortest one so
        a node could be opened more than once and expanding the \textit{dest} node doesn't
        mean that we have terminated.
\end{itemize}
\paragraph{Hash Function}
The way how the threads divide among themself the work to be done happens using
a \textit{hash function}. The hash function that we have employeed is simply
\begin{center}
  $hash(node\_index, num\_threads) = node\_index \;\%\; num\_threads$
\end{center}
As we will realize togheter this fully equal work distribution could be not the
optimal choice in a road network. 
\begin{figure}[ht!]
  \centering
  \small
  \includegraphics[width=0.5\linewidth]{hda_work_BAY.png}
  \caption{HDA* work distribution among 3 threads in BAY map}
  \label{hdawork}
\end{figure}
As we can see in figure \ref{hdawork} with 3 thread the work distribution is so equal
among the 3 threads that the way how each thread works for finding an optimal path
to destination could be not the best choice??.
\paragraph{Distributed Termination Condition}
If in the sequential algorithm expanding the node \textit{dest} triggers the end of the algorithm (despite
the fact that the \textit{open set} could be not empty) in HDA* this is not valid anymore: when a thread
is working on its \textit{open set} it is expanding nodes putting their neighbors in the
\textit{open set} of another thread. When thread $t_i$ has its \textit{open set} empty it could think
to have finished its work but this might not be true because another thread $t_j$ could be sending
to $t_i$ some nodes that have to be be processed in the meanwhile. We have employed two
different approaches for the distributed termination condition that are:
\begin{itemize}
  \item Barrier method (B): when a thread realizes that its \textit{open set} is
        empty a barrier is 
        hitten and when all the threads have hitten the barrier
        each one makes a check to confirm(or not) that all the \textit{open sets} of all
        the threads are still empty. If this is not true it means that there are nodes
        that have still to be processed and the best path to $dest$ found so far could
        not be the optimal one otherwise all the threads can terminate.
  \item Sum-Flag method (SF): the idea behind the sum flag method comes from the fact that
        the Barrier mechanism could be quite expensive. In this termination condition method
        each thread keeps a binary flag saying whether its \textit{open set} is empty
        or not. When no more nodes are inside it the flag is set and if $\sum_{i=1}^{N}flag[i] = N$
        all the threads can correctly terminate.
\end{itemize}
\paragraph{Duplicate node checking}
The loose of guarantees explained before has also to do with the \textit{closed set} handling. In particular
when a node $n$ is expanded by thread $t_i$ it is done a check wheter this node is inside the \textit{closed set}
of thread $t_i$ or not. This is not sufficient: we also need to check whether the cost associated to that node 
when it was previously added to the \textit{closed set} is less or equal to the cost computed at time $n$ is re-expanded. 
This is called duplicate checking so, a node $n$ is a duplicate of node $m$ if:
\begin{itemize} 
  \item $n$ is equal to $m$
  \item $closedSet$ of thread $t_i$ contains the node $m$
  \item $g(m) \le g(n)$
\end{itemize}
A duplicate node can be discarded.
\paragraph{Working methodology}
What it differentiates the algorithms we have implemented is not only
the distributed termination condition (B or SF) but also the way how the threads
communicate with each other. This can be done using a shared address space (SAS) approach (that will
need to cope with mutual exclusion) or a message passing (MP) model.
\subsubsection{Message Passing Model}
One way of achieving the communication among threads is message passing. This mechanism is based
on Message Queues: when thread $t_i$ expands a node and computes(through the hash function) 
its owner $t_j$ a message is sent to $t_j$. Since the message
queue is unique for all the threads each thread needs to be able to process
only messages directed to it. Each thread mantains all the data structures as private (\textit{open set},
\textit{parentVertex}, \textit{costToCome}) and the only way it has communicate with other threads
is via message passing. The termination condition that has been implemented is the Barrier Method (B).
\paragraph{Message Structure} Each message sent from $t_i$ to $t_j$ regarding node $n$ contains:
\begin{itemize}
  \item Message id: the identifier of the owner thread $t_j$
  \item The index of the node $n$
  \item The parent node of $n$ (the one expanded from the \textit{open set} of $t_i$)
  \item The value of $g(n)$ according to $t_i$
  \item The value of $f(n)$ according to $t_i$
\end{itemize}
\paragraph{Message Based Path Reconstruction}
What it makes not trivial the path reconstruction phase in the Message Passing model
is that $parentVertex$ and $costToCome$ data structures are not shared among threds. 
This means that thread $t_i$ will have inside $parentVertex_i$, once the algorithm is terminated, 
a consistent value of $parentVertex_i[n]$ only for the nodes $n$ it is the owner of. This implies that if
we want to know which is the parent node of node $n$ in the final path this information
is stored in $parentVertex_i[n]$ (where $t_i$ is the parent of $n$). Path reconstruction needs to be done in a message passing fashion starting
from the destination's thread owner $t_d$. What happens is that $t_d$ sends a message to
the owner of $parentVertex_d[dest]$ (abbreviated as $pV_d[dest]$) that we call $t_k$. Immediately after $t_k$ will send a message
to $pV_k[pV_d[dest]]$ and so on and so forth till we have reached the first node of the path (the one with $pV_x[source] = -1$ where
$t_x$ is the owner of $source$).
\subsubsection{Shared Address Space Model}
This approach applies the explained concepts of HDA* by using a communication method
among threads that exploits the shared address space (SAS). There is:
\begin{itemize}
  \item A global array of \textit{open sets} that here we call $A$ where
        $A[i]$ contains a pointer to the \textit{open set} of thread $t_i$ and the size
        of $A$ is $N$ (the number of threads).
  \item The $parentVertex$ and $costToCome$ data structures are shared among all the threads.
\end{itemize}
This approach clearly requires locks so that the operations on the shared data structures
can happen in mutual exclusion. In particular we need:
\begin{itemize}
  \item One lock $L1$ for each \textit{open set} so for each $A[i] \;\forall i \in \{1..N\}$.
  \item One lock $L2$ for each node of the graph in order to correctly update $parentVertex$ and $costToCome$ data structures.
\end{itemize}
With SAS both the barrier (B) and the sum-flag (SF) distributed termination conditions
have been implemented to compare their performances.
\begin{comment}
\SetKwComment{Comment}{/* }{ */}
\begin{algorithm*}[] 
\caption{SAS-HDA*}\label{sas}
\KwData{Graph G(V,E), Source s, Destination d, Heurstic h}
\KwResult{Best path from s to d and relative cost}
\SetKwFunction{FMain}{HDAWrapper}
  \SetKwProg{Fn}{Function}{:}{}
  \Fn{\FMain{$source$, $dest$, $N$, $heuristic$}}{
        $g[i] \gets DOUBLE\_MAX \;\forall i \in V$\;
        $h[i] \gets heuristic(i, dest) \; \forall i \in V$\;
        $parentVertex[i] \gets -1 \; \forall i \in V$\;
        $t\_owner \gets hash(source, num\_threads)$\;
        $f[t\_owner][source] \gets h[s]$\;
        $g[source] \gets 0$\;
        $openSet[t\_owner] := \{(source, f[t\_owner][source])\}$\;
        Initialize $N$ $mutex\_threads$\;
        Initialize $V$ $mutex\_nodes$\;
        Launch $N$ threads\;
        Join $N$ threads\;
        Reconstruct final path\;
        \KwRet\;
}
\SetKwFunction{FHDA}{HDA}
  \SetKwProg{Fn}{Function}{:}{}
  \Fn{\FHDA{$index$, $dest$, $N$, $heuristic$}}{
    \While{$1$}{
      \While{$!openSet[index].EMPTY()$}{
        LOCK($mutex\_threads[index]$)
        $a \gets openSet.POP()$\;
        UNLOCK($mutex\_threads[index]$)
      }
      \If($a$ is duplicate){
        continue\;
      }
      \ForEach{neighbor $b$ of $a$}{
        $wt \gets weight(a, b)$\;
        $tentativeScore \gets g[a] + wt$\;
        \If{$tentativeScore$ is less than $g[b]$}{
          $owner\_a \gets hash(a,N)$\;
          $owner\_b \gets hash(b,N)$\;
          \If($b$ is duplicate){
            continue\;
          }
          LOCK($mutex\_nodes[a]$)
          $tentativeScore \gets g[a] + wt$\;
          UNLOCK($mutex\_nodes[a]$)

          LOCK($mutex\_nodes[b]$)
          \If{$tentativeScore$ is less than $g[b]$}{
            $parentVertex[b] \gets a$\;
            $costToCome[b] \gets wt$\;
            $g[b] \gets tentativeScore$\;
            $f[b] \gets g[b] + h[b]$\;
            LOCK($mutex\_threads[owner\_b]$)
            $f[owner\_b][b] \gets f[b]$\;
            $openSet.PUSH((b, f[b]))$\;
            UNLOCK($mutex\_threads[owner\_b]$)
          }
          UNLOCK($mutex\_nodes[a]$)
        }
      }

      \If{$openSet[index].EMPTY()$ and $parentVertex[b] != -1$}{
        Terminate according to B or SF\;
      }
    }
        \KwRet\;
}
\end{algorithm*}
\end{comment}
\subsection{Results}
These are the troubles found during implementation and testing:
\begin{itemize}
  \item The SAS-MP variant of HDA* is not scalable. This is due to the fact that we
        have used Linux Message Queues that have a limited size (few bytes) and when
        the communication overhead becomes huge the queue gets full causing a impressive
        slowdown. For the sake of completeness we have decided to report the results on the
        RND map. (RND map was indeed used almost only to show the SAS-MP performaces).
  \item The SAS HDA* that exploits the Barrier termination condition (SAS-B) is more scalable
        than SAS-MP but the termination condition is not well-performing in large graphs. This 
        happens because a thread has no way to cut off the barrier if it's receiving work from
        another thread but it has to wait until all the threads have reached the barrier. Despite
        the fact that we notice an improvement when the number of threads increases the execution
        time is too large from map W on.
  \item The SAS HDA* that exploits the Sum-Flag termination condition (SAS-SF) behaves overall better.
        Despite the fact on maps BAY, FLA, W it is difficult to notice the improvements when
        the number of threads increase this is more evident on USA map. Performances are always
        better compared to the SAS-B algorithm.
  \item TODO resource considerations (not meaningful on RND map)
\end{itemize}
HDA* has been proven not to work well in our application. We have tried to explain thse 
results by observing that:
\begin{itemize}
  \item The hash function we have adopted could not be optimal for this type of problem. It
        is happening that instead of helping the algorithm in exploring the nodes towards the destination
        we are making its work more complex by randomly assigning the work to the different threads.
        What could be helpfull would be choosing another strategy (another hash function) able to 
        divide the work in a more reasonable way among threads (for instance tring to assign near nodes
        to the same threads maybe by pre-clustering the nodes of the map in a number of clusters
        equal to the number of threads). This could lead to obtain better performances but we
        suppose that the sequential algorithm would still be the "best case" instead of the "worst case"
        in terms of execution time.
  \item The termination condition is clearly a bottleneck, in particular if the Barrier method
        is used. This can be less evident with a small maps but we can appreciate it on bigger graphs.
        Anyway, having more threads improves time execution but has the drawback of consuming more
        memory and CPU.      
\end{itemize}
\begin{figure}[ht!]
  \centering
  \includegraphics[width=1\linewidth]{hda/rnd.png}
  \caption{HDA* on RND map}
  \label{hdarnd}
\end{figure}
\begin{figure}[ht!]
  \centering
  \includegraphics[width=1\linewidth]{hda/bay.png}
  \caption{HDA* on BAY map}
  \label{hdabay}
\end{figure}
\begin{figure}[ht!]
  \centering
  \includegraphics[width=1\linewidth]{hda/fla.png}
  \caption{HDA* on FLA map}
  \label{hdafla}
\end{figure}
\begin{figure}[ht!]
  \centering
  \includegraphics[width=1\linewidth]{hda/w.png}
  \caption{HDA* on W map}
  \label{hdaw}
\end{figure}
\begin{figure}[ht!]
  \centering
  \includegraphics[width=1\linewidth]{hda/usa.png}
  \caption{HDA* on USA map}
  \label{hdausa}
\end{figure}
\begin{figure}[ht!]
  \centering
  \includegraphics[width=1\linewidth]{hda/tot.png}
  \caption{HDA* overall performances}
  \label{hdatot}
\end{figure}

\subsection{Parallel Bidirectional Search}
(Explanation + Pseudocode)
\subsubsection{Results}
(Plots to show thread 1 and 2 works, plots of time)

\section{Complete Results}
(Tables with numbers)

\section{Final Considerations}
(Comments)

\section{DIMACS Benchmark}
(More detailed explanation of the input format of the benchmarks)

\section{Future Works}
(Possible improvements)




%%%%%%%%%%%% Supplementary Methods %%%%%%%%%%%%
%\footnotesize
%\section*{Methods}

%%%%%%%%%%%%% Acknowledgements %%%%%%%%%%%%%
%\footnotesize
%\section*{Acknowledgements}

%%%%%%%%%%%%%%   Bibliography   %%%%%%%%%%%%%%
\normalsize
\bibliography{references}

%%%%%%%%%%%%  Supplementary Figures  %%%%%%%%%%%%
%\clearpage

%%%%%%%%%%%%%%%%   End   %%%%%%%%%%%%%%%%
%\end{multicols}  % Method B for two-column formatting (doesn't play well with line numbers), comment out if using method A
\end{document}