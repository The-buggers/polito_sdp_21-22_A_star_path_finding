%% arara directives
% arara: xelatex
% arara: bibtex
% arara: xelatex
% arara: xelatex

%\documentclass{article} % One-column default
\documentclass[twocolumn, switch]{article} % Method A for two-column formatting

\usepackage{preprint}
\newcommand{\removelatexerror}{\let\@latex@error\@gobble}
%% Math packages
\usepackage{amsmath, comment, amsthm, amssymb, amsfonts, algorithm2e, algpseudocode, multicol,  graphicx, caption, subcaption, multirow}
\RestyleAlgo{ruled}
%\SetAlFnt{\scriptsize}
\graphicspath{ {./Pictures/} }
%% Bibliography options
\usepackage[numbers,square]{natbib}
\bibliographystyle{unsrtnat}
%\usepackage{natbib}
%\bibliographystyle{Geology}

%% General packages
\usepackage[utf8]{inputenc}	% allow utf-8 input
\usepackage[T1]{fontenc}	% use 8-bit T1 fonts
\usepackage{xcolor}		% colors for hyperlinks
\usepackage[colorlinks = true,
            linkcolor = purple,
            urlcolor  = blue,
            citecolor = cyan,
            anchorcolor = black]{hyperref}	% Color links to references, figures, etc.
\usepackage{booktabs} 		% professional-quality tables
\usepackage{nicefrac}		% compact symbols for 1/2, etc.
\usepackage{microtype}		% microtypography
\usepackage{lineno}		% Line numbers
\usepackage{float}			% Allows for figures within multicol
%\usepackage{multicol}		% Multiple columns (Method B)

\usepackage{lipsum}		%  Filler text

 %% Special figure caption options
\usepackage{newfloat}
\DeclareFloatingEnvironment[name={Supplementary Figure}]{suppfigure}
\usepackage{sidecap}
\sidecaptionvpos{figure}{c}

% Section title spacing  options
\usepackage{titlesec}
\titlespacing\section{0pt}{12pt plus 3pt minus 3pt}{1pt plus 1pt minus 1pt}
\titlespacing\subsection{0pt}{10pt plus 3pt minus 3pt}{1pt plus 1pt minus 1pt}
\titlespacing\subsubsection{0pt}{8pt plus 3pt minus 3pt}{1pt plus 1pt minus 1pt}

% ORCiD insertion
\usepackage{tikz,xcolor,hyperref}

\definecolor{lime}{HTML}{A6CE39}
\DeclareRobustCommand{\orcidicon}{
	\begin{tikzpicture}
	\draw[lime, fill=lime] (0,0) 
	circle [radius=0.16] 
	node[white] {{\fontfamily{qag}\selectfont \tiny ID}};
	\draw[white, fill=white] (-0.0625,0.095) 
	circle [radius=0.007];
	\end{tikzpicture}
	\hspace{-2mm}
}
\foreach \x in {A, ..., Z}{\expandafter\xdef\csname orcid\x\endcsname{\noexpand\href{https://orcid.org/\csname orcidauthor\x\endcsname}
			{\noexpand\orcidicon}}
}
% Define the ORCID iD command for each author separately. Here done for two authors.
\newcommand{\orcidauthorA}{0000-0000-0000-0001}
\newcommand{\orcidauthorB}{0000-0000-0000-0002}
\newcommand{\orcidauthorC}{0000-0000-0000-0003}
\newcommand{\orcidauthorD}{0000-0000-0000-0004}

%%%%%%%%%%%%%%%%   Title   %%%%%%%%%%%%%%%%
\title{Parallel A* project}

% Add watermark with submission status
\usepackage{xwatermark}
% Left watermark
\newwatermark[firstpage,color=gray!60,angle=90,scale=0.32, xpos=-4.05in,ypos=0]{\href{https://doi.org/}{\color{gray}{Publication doi}}}
% Right watermark
\newwatermark[firstpage,color=gray!60,angle=90,scale=0.32, xpos=3.9in,ypos=0]{\href{https://doi.org/}{\color{gray}{Preprint doi}}}
% Bottom watermark
\newwatermark[firstpage,color=gray!90,angle=0,scale=0.28, xpos=0in,ypos=-5in]{*correspondence: \texttt{email@institution.edu}}

%%%%%%%%%%%%%%%  Author list  %%%%%%%%%%%%%%%
\usepackage{authblk}
\renewcommand*{\Authfont}{\bfseries}
\author{Lorenzo Ippolito, Mattia Rosso, Fabio Mirto}


%%%%%%%%%%%%%%    Front matter    %%%%%%%%%%%%%%
\begin{document}

\twocolumn[ % Method A for two-column formatting
  \begin{@twocolumnfalse} % Method A for two-column formatting
  
\maketitle

\begin{abstract}
  ...Here the abstract...
\end{abstract}
%\keywords{First keyword \and Second keyword \and More} % (optional)
\vspace{0.35cm}

  \end{@twocolumnfalse} % Method A for two-column formatting
] % Method A for two-column formatting

%\begin{multicols}{2} % Method B for two-column formatting (doesn't play well with line numbers), comment out if using method A


%%%%%%%%%%%%%%%  Main text   %%%%%%%%%%%%%%%
% \linenumbers

%% %%%%%%%%%%%%%%%%%%%%%%% %%
%% CHAPTER 1: INTRODUCTION %%
%% %%%%%%%%%%%%%%%%%%%%%%% %%
\section{Introduction: about the A* algorithm}
A* is a graph-traversal and path-search algorithm. It is used in many contexts of computer science and 
not only. It can be considered as a general case of the Dijkstra algorithm and it achieves better performaces
with respect to it. It is a Greedy-best-first-search algorithm that uses an heuristic function to guide
itself. What it does is combining:
\begin{itemize}
  \item Dijkstra approach: favore nodes closed to the starting point(source)
  \item Greedy-best-first-search approach: favore nodes closed to the final point(destination)
\end{itemize}
Using the standard terminology we have:
\begin{itemize}
  \item $g(n)$: exact cost of moving from source to n
  \item $h(n)$: heuristic estimated cost of moving from a node n(source included) to the destination
  \item $f(n) = g(n) + h(n)$: in this way we are able to combine the actual cost with the estimated one
\end{itemize}
At each (main loop) iteration the node $n$ that has the minimum $f(n)$ is examinated.
\subsection{Heuristic design}
\paragraph{Premises} We are going to work with weighted oriented graph where $V$ is the set on nodes/vertices
and $E$ is the set of edges of the form $(x,y)$ to indicate that an oriented edge from $x$ to $y$ exists and it
has weight $cost(x, y)$.
\paragraph{Heuristic properties} 
The heurstic function represents the acutal core of the A* algorithm. It represents a prior-knowledge that
we have about the cost of the path from every node (source included) to the destination. 
\begin{itemize}
  \item If we have not this prior information($h(n) = 0 \;\forall n \in V$) we are turning the A* algorithm into
        Dijkstra (this is why A* can be considered as a more general case of Dijkstra algorithm) but we always
        have the guarantee of finding the shortest path. 
  \item Admissible Heuristic: if $h(n) < cost(n, dest) \;\forall n \in V$ (so the we never over-estimate the distance to
        get to the destination from a node $n$) A* will always find the shortest path and the heuristic
        function is called \textit{admissible}. The more inaccurate
        is the estimation the more nodes A* will need to expand (with the upper bound of expanding every
        nodes in the graph if $h(n) = 0$).
  \item Consistent Heuristic: if $h(x) \le cost(x, y) + h(y)$ for every edge $(x, y)$ (so the triangular
        inequality is satisfied) A* has the guarantee to find an optimal path without processing
        any node more than once. 
\end{itemize}
\paragraph{Corner cases} 
\begin{itemize}
  \item Dijkstra: As already discussed if $h(n)=0$ for every node in the graph A* turns into the Dijkstra algorithm.
  \item Ideal: We would obtain a perferct behaviour in case $h(n)$ is exactly equal to the cost of moving from $n$ to
  the destination (A* will only expand the nodes on the best path to get to the destination).
  \item Full greedy-best-first search: if $h(n) \gg g(n)$ than only $h(n)$ plays a role and A* turns into
        a completly greedy-best-first search algorithm.
\end{itemize}

%% %%%%%%%%%%%%%%%%%%%%%% %%
%% CHAPTER 2: APPLICATION %%
%% %%%%%%%%%%%%%%%%%%%%%% %%
\section{A* project application}
\paragraph{Problem definition} Given a wide range of fields where the A* algorithm can be applied we have choosed the one of
optimal path searching in geographical areas where the goal is to find the minimum distance path from a 
node source to a node destination.

\paragraph{Notation} We work with a weighted oriented graph $G$ that is made of nodes $n \in V$ that represents
road-realated points of interest and edges $(x,y) \in E$ represent the unidirectional connections among these points.
Each edge $(x, y)$ is associated to a weight that is the great-circle distance between $x$ and $y$ measured
in meters.
\paragraph{Benchmark} We will exploit the DIMACS benchmark to make robust estimates of the designed algortithms.
Starting from the FIPS system format files provided we have adopted them (as better explained in section ...) to provide
to the algorithms a file containing information structured as:
\begin{itemize}
  \item Nodes: each node $n$ is defined as $(index, longitude, latitude)$ where $index$ is a natural progressive
        number starting from $0$ used to univocally identify the node and $(longitude, latitude)$ are the geographical
        coordinates of the node.
  \item Edges: each edge $(x, y)$ is defined as $(x, y, weight)$ that represent a unidirectional connection
        from $x$ to $y$ (a road) with length $weight$ (great-circle distance from $x$ to $y$).
\end{itemize}
\subsection{Heuristic function: the great-circle distance}
As previoulsly discussed the A* needs an admssible and consistent heuristic to properly work and this
function is tipically problem-specific. Given the type of problem we are going to apply A* to we are going
to use a measure of geographical distance that extends the concept of heuclidean distance between two points:
the great circle distance (that is the shortest distance over the earth surface measured along the
earth surface itself).
\begin{figure}[ht!]
  \centering
  \includegraphics[width=0.5\linewidth]{haversine.png}
  \caption{Great circle distance from P to Q}
  \label{haversine}
\end{figure}
We will employee the Haversine formula to compute the distance from node $(\phi_1,\lambda_1)$
and $(\phi_1,\lambda_1)$ where $\phi$ is the latitude and $\lambda$ is the longitude:
\begin{center}
  $d = R \cdot c$\\
  where $c = 2 \cdot atan2(\sqrt{a},\sqrt{1-a})$\\
  $a = sin^2\Big({\frac{\Delta \phi}{2}}\Big) + cos(\phi_1) \cdot cos(\phi_2) \cdot sin^2\Big({\frac{\Delta \lambda}{2}}\Big)$
  \\$R$ is the earth radius that we have fixed to $R=6.371km$
\end{center}

%% %%%%%%%%%%%%%%%%%%%%%% %%
%% CHAPTER 1: INPUT GRAPH %%
%% %%%%%%%%%%%%%%%%%%%%%% %%
\section{Graph file input}
\paragraph{File input format} 
Now we start discussing the A* alogorithm implementation and to do that we need to specify which 
types of files we will need to provide to the algorithm to load the graph of interest. Each file has 
the format:
\begin{itemize}
    \item First line: the number of nodes $N[int]$
    \item N following lines: nodes appearing as $(index[int], longitude[double], latidue[double])$
    \item E following lines (with E unknown): edges appearing as $(x[int], y[int], weight[double])$
\end{itemize}
\paragraph{Random test graph} We have tested the designed algorithms also a random generated graph that
is build starting from: 
\begin{itemize}
  \item Which path we want to find: given the couple (source, destination) it is generated a graph of
        $max(source, destination) + 1$ nodes.
  \item How many paths at most have to be generated from source to destination
  \item The maximum length of these paths (that will be randomly chosen for each path)
\end{itemize}
In this way we have ad-hoc files to stress the algorithm having the guarantee that more than one path
exists from source to destination (other more standart approach exist for random graph generation but we
have implemented this custom one for this reason). To be consistent with benchmark files also these
random graphs represents geographic points with longitude and latitude.
\paragraph{DIMACS benchmark} 
The benchmark files we have used come from the DIMACS benchmark. Here each geographic map is described by:
\begin{itemize}
  \item \textit{.co} file: a file containing the coordinates of the nodes following the FIPS system notation
  \item \textit{.gr} file: a file containing the edges and the relative weight(distance) expressed in meters
\end{itemize}
The generation of a file consistent with the format described above happens by merging these files 
into a new one (in binary format). One of the challenge we are going to undertake is the one of
parallelizing the reading of these huge files (that we will show having an high impact in terms of
execution time over the overall A* algorithm).
\paragraph{Test paths}
To analyze the performance of the different versions of A* algorithms we have used these paths:
\begin{table}[ht!]
	\caption{Test paths for A*}
	\centering
	\begin{tabular}{ |l|l|l|l| }
		\hline
		\textit{Nodes} & \textit{Edges} & \textit{Source} & \textit{Dest} \\ 
    \hline
		\multicolumn{4}{ |c| }{\bf{Random map}} \\
		\hline
		\multirow{1}{*}{}
		 101& - & 0 & 100\\
		\hline
		\multicolumn{4}{ |c| }{\bf{California(BAY)}} \\
		\hline
		\multirow{1}{*}{}
		321270& 800172 & 321269 & 263446\\
		\hline
		\multicolumn{4}{ |c| }{\bf{Florida(FLA)}} \\
		\hline
		\multirow{1}{*}{}
		1070376 & 2712798 & 0 & 103585\\
		\hline
    \multicolumn{4}{ |c| }{\bf{Western USA(W)}} \\
		\hline
		\multirow{1}{*}{}
		6262104 & 15248146 & 1523755 & 1953083\\
		\hline
    \multicolumn{4}{ |c| }{\bf{Full USA(USA)}} \\
		\hline
		\multirow{1}{*}{}
		23947347 & 58333344 & 14130775 & 810300\\
		\hline
	\end{tabular}
\end{table}
The paths on benchmark graphs BAY and FLA have been choosen ad-hoc to traverse the entire map with
a reasonable long path.
\begin{figure}[ht!]
  \centering
  \includegraphics[width=0.75\linewidth]{google_maps.png}
  \caption{Test paths on BAY(left) and FLA(right)}
  \label{testpaths}
\end{figure}

%% %%%%%%%%%%%%%%%%%%%%%%%% %%
%% CHAPTER 1: A* SEQUENTIAL %%
%% %%%%%%%%%%%%%%%%%%%%%%%% %%
\section{Sequential A* Algorithm}
Sequential A* algorithm is the one we will start with to see how the algorithm works and performs.
The first step consists of a pre-computation of:
\begin{itemize}
  \item The heurstic $h(n)$ for each node (by defintion the $h(dest)$ will
        be 0) computed through the Haversine formula. We thus keep a global data
        structure \texttt{h} to do this.
  \item The initial values of $f(n)$ and $g(n)$ that will be set to $DOUBLE\_MAX$ for each
        node except for the source node that will have $f(source) = h(source)$ because 
        $g(soruce)$ is clearly $0$. We thus keep global data structures \texttt{f} and \texttt{g} to do this. 
        $f(source) = g(source) + h(source) = 0 + h(source) = h(source)$.
\end{itemize}
We will also need two additional global data structures:
\begin{itemize}
  \item The global \textit{costToCome} table (where \texttt{costToCome[i]} contains the current
        best cost to reach node \texttt{i}) that is initialized
        to $DOUBLE\_MAX$.
  \item The global \textit{parentVertex} table (where \texttt{parentVertex[i]} contains the
        parent node of node \texttt{i} according to the current best path found to reach
        the destination) that is initialized with $-1$.
\end{itemize}
The outer loop is based on the nodes extraction from a \textit{open set}, the one containing
the nodes that have still to be explored (it is implemented as a Priory Queue where the priority
is associated to the $f(n)$ of the nodes). At each iteration the node $a$ with minimum $f(a)$ is
extracted from the \textit{open set} and its neighbors are expanded: the inner loop is repeated once for each neighbor $b$ of the
extracted node $a$($b$ is neighbor of $a$ if the edge $(a, b)$ exists). A tentative score is computed
for each node $b$ as $g(b) = g(a) + weight(a, b)$. If $g(b)$ is less than current \texttt{g[b]} these data structures
are updated: 
\begin{flushleft}
  \texttt{g[b] = g[a] + weight(a, b)}\\
  \texttt{costToCome[b] = g[b]}\\
  \texttt{parentVertex[b] = a}\\
  \texttt{f[b] = g[b] + h[b]}
\end{flushleft}
The final step of this inner loop is checking wheter the node $b$ has already been added to the 
\textit{open set} or not. In case the \textit{open set} doesn't contain it we add it (with priority
$f(n)$ just computed).
Since the heuristic function we have chosen is both admissible and consistent we have the guarantee 
that the first time a node is extracted from the \textit{open set} we have found a best path to it (this
is why when the destination node is extracted we can terminate having found the best path). So despite
the node may be added to the \textit{open set} more than once (when discovered as a neighbor of different nodes)
it will be only expanded once.
\subsection{Pseudocode}
\SetKwComment{Comment}{/* }{ */}
\begin{algorithm}[ht!] 
\caption{Sequential A*}\label{alg:two}
\KwData{Graph G(V,E), Source s, Destination d, Heurstic h}
\KwResult{Best path from Source to Destination and relative cost}
$g[i] \gets DOUBLE\_MAX \;\forall i \in V$\;
$f[i] \gets DOUBLE\_MAX \;\forall i \in V$\;
$h[i] \gets h(i, d) \; \forall i \in V$\;
$costToCome[i] \gets 1 \; \forall i \in V$\;
$parentVertex[i] \gets 1 \; \forall i \in V$\;
$f[s] \gets h[s]$\;
$g[s] \gets 0$\;
$openSet := \{(s, f[s])\}$\;
\While{$!openSet.EMPTY()$}{
  $a \gets openSet.POP()$\;
  \If{$a == d$}{
    $pathFound \gets true$\;
    reconstructPath()\;
  }
  %\Comment*[r]{Inner loop}
  \ForEach{neighbor $b$ of $a$}{
    $wt \gets weight(a, b)$\;
    $tentativeScore \gets g[a] + wt$\;
    \If{$tentativeScore$ is less than $g[b]$}{
      $parentVertex[b] \gets a$\;
      $costToCome[b] \gets wt$\;
      $g[b] \gets tentativeScore$\;
      $f[b] \gets g[b] + h[b]$\;
      $openSet.PUSH((b, f[b]))$\;
    }
  }
}
\end{algorithm}

\subsection{Results}
We are now going to run the sequential A* algorithm.
\begin{table}[]
  \caption{Sequential algorithms performance}
  \begin{tabular}{|l|l|l|l|l|l|}
  \hline
  \textbf{}    & \textbf{File Size} & \textbf{Reading} & \textbf{A*} & \textbf{Total} & \textbf{\begin{tabular}[c]{@{}l@{}}Reading \\ Impact\end{tabular}} \\ \hline
  \textbf{RND} & 2876B              & 0.0011s          & 0.0862s     & 1.1714s        & 1.2872\%                                                           \\ \hline
  \textbf{BAY} & 20.51MB            & 0.9365s          & 0.2349s     & 1.1714s        & 79.9477\%                                                          \\ \hline
  \textbf{FLA} & 69.09MB            & 3.0728s          & 0.5893s     & 3.6621s        & 83.9075\%                                                          \\ \hline
  \end{tabular}
\end{table}
We can realize that (despite for the random graph that is too small to appreciate this result) the 
reading time has a very high impact on the overall execution time of the algorithm and in section 6
we will investigate three different techniques for parallelizing the reading of the file. 
\section{A* and Dijkstra: a comparison}
As already mentioned the Dijkstra algorithm can be considered as a particular case of A* where we
don't any prior knowledge about the distances of the nodes (heuristic $h(n) = 0 \;\forall n \in V$).
We have also seen that the more precise is the heuristic function we provide the less nodes the algorithm
will expand to get to the destination obtaining the best path. To investigate this point we have run
Dijkstra algorithm on the same graph (here we only consider BAY and FLA) comparing the number of 
expanded nodes by the two algorithms:

\begin{table}[ht!]
  \caption{Expanded nodes vs total nodes for Dijkstra and A*}
  \begin{tabular}{|l|l|l|}
  \hline
  \textbf{Expanded nodes} & \textbf{Dijkstra} & \textbf{Sequential A*}       \\ \hline
  \textbf{RND}            & 15 of 101         & 13 of 101         \\ \hline
  \textbf{BAY}            & 318725 of 321270  & 156950 of 321270  \\ \hline
  \textbf{FLA}            & 996956 of 1070376 & 591926 of 1070376 \\ \hline
  \end{tabular}
\end{table}

The number of expanded nodes is clearly much higher when Dijkstra algorithm is used and the picture
\ref{astardijkstra} cleary show in \textcolor{blue}{blue} the nodes expanded by the sequntial A* algorithm
while the \textcolor{red}{red} show the nodes expanded by the Dijkstra algorithm. In the BAY map Dijkstra
expands $50\%$ nodes more than A* while in FLA it is less evident since Dijkstra expands $38\%$ of nodes
more than A*.
\begin{figure}[ht!]
  \centering
  \includegraphics[width=1\linewidth]{dijkstra_astar.png}
  \caption{Test paths on BAY(left) and FLA(right)}
  \label{astardijkstra}
\end{figure}

\section{Parallel reading of the input file}
As we have seen in section 5 the time spent by the algorithm to read the input graph
(the input binary file) used by the A* algorithm is very high w.r.t. the total
amount of time spent. This is the reason why we have decided to inspect three techniques of
parallelization of the reading phase to speed it up. The input file, as already discussed,
is divided in two different sections (nodes and edges) so, in general, we need to take care of which
section a given thread is working on because different data structures of the graph need to
be loaded in the two sections (the symbol table when reading a \textit{node line} and the linked list when
reading a \textit{edge line}).
\subsection{Parallel Read: approach 1}
In this first approach we have implemeted a solution on which:
\begin{itemize}
  \item $N$ threads runs freely to read the entire file
  \item Only one file descriptor is shared among all the threads (this means that
        when thread $t_i$ performs a \texttt{read} opearation all the other threads 
        are waiting for it to finish)
\end{itemize}
This is the most trivial solution that can be adopted and the drawback is the really high
resource contention that exists among all the threads (when N threads whant to access line
$k$ of the file only $1$ can do it and others $N-1$ wait). The advantage is that the file
will be read \textit{sequentially} but in a multithread fashion (line $k$ of the file
is always read before line $k+1$) and this results in a easier implementation.
\paragraph{Results on FLA}
\begin{figure}[ht!]
  \centering
  \includegraphics[width=1\linewidth]{par_read_1_time.png}
  \caption{Performance of approach 1 for different number of threads}
  \label{parread1time}
\end{figure}
\subsection{Parallel Read: approach 2}
(Explanation)
\paragraph{Results on BAY}
\begin{figure}[ht!]
  \centering
  \includegraphics[width=1\linewidth]{par_read_2_time.png}
  \caption{Performance of approach 2 for different number of threads}
  \label{parread2time}
\end{figure}
\subsection{Parallel Read: approach 3}
This is the approach that generalize the first one by:
\begin{itemize}
  \item Letting threads differentiate among \textit{nodes section} and 
        \textit{edges section} to be able to read them togheter.
  \item Using a $(NP, NT)$ mechanism to read each section of the input file (where
        $NP$ is the number of partitions the section is divided in and $NT$ is the number
        of threads that have to read all the partitions).
\end{itemize}
\paragraph{The $(NP, NT)$ mechanism}
We need to provide as input:
\begin{itemize}
  \item $NP-nodes$: the number of partitions of the \textit{nodes section}
  \item $NP-edges$: the number of partitions of the \textit{edges section}
  \item $NT-nodes$: the number of threads that have to read the \textit{nodes section}
  \item $NT-edges$: the number of threads that have to read the \textit{edges section}
\end{itemize}
If the number of threads is equal to the number of partitions we have a simpler mechanism in
which each thread will read only one partition and then terminates.
As we can see in the example in figure \ref{parread3} threads will iterate over the partitions
of the sections that have been statically allocated to them. This means that there is not a real
contention of the reading phase since the partions are not overlapping and
each partition will be read by one and only one thread (actually, the
loading of the graph data structures after reading each line must be done in mutual exclusion so we will need a lock
both for the \textit{nodes-threads} and for the \textit{edges-threads}).
\begin{figure}[ht!]
  \centering
  \includegraphics[width=1\linewidth]{par_read_3.png}
  \caption{Example of parallel read - approach 3}
  \label{parread3}
\end{figure}
Each thread terminates when it has no more partitions to read.\\
\paragraph{Results on FLA}
\begin{figure}[ht!]
  \centering
  \includegraphics[width=1\linewidth]{par_read_3_time.png}
  \caption{Performance of approach 3 for different number of threads and partitions}
  \label{parread3time}
\end{figure}
We can realize from figure \ref{parread3time} that the number of partitions (setted equal
both for \textit{nodes section} and \textit{edges section}) has not a high impact.
What has more impact is the number of threads, an increasing number of threads gives increasing
bad performances while with a small number of threads we are able to obtain better results
w.r.t the first approach but worse result w.r.t the sequential reading. The reason of this
behaviour could be due to the fact that there is not a completly parallel work of the threads
since the access to Graph's data structure has to be done in mutual exclusion so we 
have benefits in dividing the threads in \textit{edges} threads and \textit{nodes} threads (something
that didn't happen in the first approach) but the mutual exclusion access to the Graph
makes the overhead due to threads creation and management not being able to obtain good
performances
\subsection{Final results}
\begin{figure}[ht!]
  \centering
  \includegraphics[width=1\linewidth]{par_read_all_time.png}
  \caption{RA(Read Approach) 1,2,3 compared with sequential reading}
  \label{parreadalltime}
\end{figure}
For the sake of completeness we have also test the most promising reading approach
on all the three maps (BAY, FLA, USA) and results are showed in table \ref{readresultsfinal}
where the speed-up with respect to the sequential reading time is evident.
\begin{table}[ht!]
  \centering
    \caption{RA2 results against sequential reading}
    \begin{tabular}{|l|l|l|l|}
    \hline
    \textbf{}    & \textbf{\begin{tabular}[c]{@{}l@{}}RA2\\ (2 threads)\end{tabular}} & \textbf{Sequential} & \textbf{Speed-Up} \\ \hline
    \textbf{BAY} & 0.1936s                                                            & 0.9366s             & 79.3\%            \\ \hline
    \textbf{FLA} & 0.5103s                                                            & 3.0650s             & 83.4\%            \\ \hline
    \textbf{USA} & 14.1492s                                                           & 56.4445s            & 74.9\%            \\ \hline
    \end{tabular}
    \label{readresultsfinal}
\end{table}

\section{Parallel A*: three examinated approaches}
The goal of the project was to find one or more parallel versions of the A* algorithm 
and showing their performances w.r.t. the sequential version. We have choosen three approaches
to face the problem of parallelizing the A* algorithm: a trivial one that simply
works as the sequential algorithm but gives the possibility of executing it in a multithread
fashion by sharing the common data structure (the \textit{OPEN SET}) among a variable number
of threads; a second one that (known in literature as HDA*) that puts in action a more
complex way of parallelizing the algorithm by defining a hash-based work distribution strategy and
finally a third approach that makes use of the parallelism in order to make the work of
finding the shortest path from \textit{source} to \textit{dest} divided between only two threads
where one looks for the path from \textit{source} to \textit{dest} and the other one looks
for the path from \textit{dest} to \textit{source} in the reversed graph (the MBA* algorithm).

\subsection{First Attempt In Parallelizing A*}

\subsection{HDA*}
The Hash-Distributed-A* (HDA*) algorithm works in a completly different way w.r.t. the 
first attempt algorithm. It is based on the fact that each thread is \textit{owner} of a
specific set of nodes of the Graph: given a node $n$ it is defined a hash function $f:\;f(n) = t$
where $t \in \{1..N\}$ with $N$ the number of threads. When a thread extracts from the \textit{OPEN SET}
(expands) a node all its neighbors are added to the \textit{OPEN SET} of the owner thread. One important
fact is that HDA* doesn't provide the same guarantees of the sequential algorithm:
\begin{itemize}
  \item In sequential A* if it's provided an heuristic function that is both \textit{admissible}
        and \textit{consistent} we have the guarantee that each node will be only expanded once
        and that the first time we expand that node we have found a shortest path to it.
  \item In HDA* we loose these guarantees: since we don't know in which order nodes will be processed
        it could happen that a longest path to \textit{dest} is found before the shortest one so
        a node could be opened more than once and expanding the \textit{dest} node doesn't
        mean that we have terminated.
\end{itemize}
\paragraph{Hash Function}
The way how the threads divide among themself the work to be done happens using
a \textit{hash function}. The hash function that we have employeed is simply
\begin{center}
  $hash(node\_index, num\_threads) = node\_index \;\%\; num\_threads$
\end{center}
As we will realize togheter this fully equal work distribution could be not the
optimal choice in a road network. 
\begin{figure}[ht!]
  \centering
  \small
  \includegraphics[width=0.5\linewidth]{hda_work_BAY.png}
  \caption{HDA* work distribution among 3 threads in BAY map}
  \label{hdawork}
\end{figure}
As we can see in figure \ref{hdawork} with 3 thread the work distribution is so equal
among the 3 threads that the way how each thread works for finding an optimal path
to destination could be not the best choice??.
\paragraph{Distributed Termination Condition}
If in the sequential algorithm expanding the node \textit{dest} triggers the end of the algorithm (despite
the fact that the \textit{OPEN SET} could be not empty) in HDA* this is not valid anymore: when a thread
is working on its \textit{OPEN SET} it is expanding nodes putting their neighbors in the
\textit{OPEN SET} of another thread. When thread $t_i$ has its \textit{OPEN SET} empty it could think
to have finished its work but this might not be true because another thread $t_j$ could be sending
to $t_i$ some nodes that have to be be processed in the meanwhile. We have employed two
different approaches for the distributed termination condition that are:
\begin{itemize}
  \item Barrier method (B): when a thread realizes that its \textit{OPEN SET} is
        empty and the cost to reach $dest$ is less than $DOUBLE_MAX$ a barrier is 
        hitten and when all the threads have hitten the barrier
        each one makes a check to confirm(or not) that all the \textit{OPEN SETs} of all
        the threads are still empty. If this is not true it means that there are nodes
        that have still to be processed and the best path to $dest$ found so far could
        not be the optimal one otherwise all the threads can terminate.
  \item Sum-Flag method (SF): the idea behind the sum flag method comes from the fact that
        the Barrier mechanism could be quite expensive. In this termination condition method
        each thread keeps a binary flag saying whether its \textit{OPEN SET} is empty
        or not. When no more nodes are inside it the flag is set and if $\sum_{i=1}^{N}flag[i] = N$
        we can terminate.
\end{itemize}
What it differentiates the algorithms we have implemented is not only
the distributed termination condition (B or SF) but also the way how the threads
communicate with each other. This can be done using a shared address space (SAS) approach (that will
need to cope with mutual exclusion) or a message passing (MP) model.
\subsubsection{Message Passing Model}
One way of achieving the communication among threads is message passing. This mechanism is based
on Message Queues: when thread $t_i$ expands a node and computes(through the hash function) 
its owner $t_j$ a message is sent to $t_j$. Since the message
queue is unique for all the threads each thread needs to be able to process
only messages directed to it. Each thread mantains all the data structures as private (\textit{OPEN SET},
\textit{parentVertex}, \textit{costToCome}) and the only way it has communicate with other threads
is via message passing. The termination condition that has been implemented is the Barrier Method (B).
\paragraph{Message Structure} Each message sent from $t_i$ to $t_j$ regarding node $n$ contains:
\begin{itemize}
  \item Message id: the identifier of the owener thread $t_j$
  \item The index of the node $n$
  \item The parent node of $n$ (the one expanded from the \textit{OPEN SET} of $t_i$)
  \item The value of $g(n)$ according to $t_i$
  \item The value of $f(n)$ according to $t_i$
\end{itemize}
\paragraph{Message Based Path Reconstruction}
What it makes not trivial the path reconstruction phase in the Message Passing model
is that $parentVertex$ and $costToCome$ data structures are not shared but they are private
for each thread. This means that thread $t_i$ will have inside $parentVertex_i$, once the algorithm is terminated, 
a consistent value of $parentVertex_i[n]$ only for the nodes $n$ it is the owner of. This implies that if
we want to know which is the parent node of node $n$ in the final path this information
is stored in $parentVertex_i[n]$ (where $t_i$ is the parent of $n$). Path reconstruction needs to be done in a message passing fashion starting
from the destination's thread owner $t_d$. What happens is that $t_d$ sends a message to
the owner of $parentVertex_d[dest]$ (abbreviated as $pV_d[dest]$) that we call $t_k$. Immediately after $t_k$ will send a message
to $pV_k[pV_d[dest]]$ and so on and so forth till we have reached the first node of the path (the one with $pV_x[firstnode] = -1$ where
$t_x$ is the owner of $firstnode$).
\subsubsection{Shared Address Space Model}
This approach applies the explained concepts of HDA* by using a communication method
among threads that exploits the shared address space (SAS). There is:
\begin{itemize}
  \item A global array of \textit{OPEN SETs} that here we call $A$ where
        $A[i]$ contains a pointer to the \textit{OPEN SET} of thread $t_i$ and the size
        of $A$ is $N$ (the number of threads).
  \item The $parentVertex$ and $costToCome$ data structures are shared among all the threads.
\end{itemize}
This approach clearly requires locks so that the operations on the shared data structures
can happen in mutual exclusion. In particular we need:
\begin{itemize}
  \item One lock $L1$ for each \textit{OPEN SET} so for each $A[i] \;\forall i \in \{1..N\}$.
  \item One lock $L2$ for each node of the graph in order to correctly update $parentVertex$ and $costToCome$ data structures.
\end{itemize}
With SAS both the barrier (B) and the sum-flag (SF) distributed termination conditions
have been implemented to compare their performances.
The pseudocode in \ref{sas} better details all the opearations.
\subsection{Pseudocode}
\begin{comment}
\SetKwComment{Comment}{/* }{ */}
\begin{algorithm*}[] 
\caption{SAS-HDA*}\label{sas}
\KwData{Graph G(V,E), Source s, Destination d, Heurstic h}
\KwResult{Best path from s to d and relative cost}
\SetKwFunction{FMain}{HDAWrapper}
  \SetKwProg{Fn}{Function}{:}{}
  \Fn{\FMain{$source$, $dest$, $N$, $heuristic$}}{
        $g[i] \gets DOUBLE\_MAX \;\forall i \in V$\;
        $h[i] \gets heuristic(i, dest) \; \forall i \in V$\;
        $parentVertex[i] \gets -1 \; \forall i \in V$\;
        $t\_owner \gets hash(source, num\_threads)$\;
        $f[t\_owner][source] \gets h[s]$\;
        $g[source] \gets 0$\;
        $openSet[t\_owner] := \{(source, f[t\_owner][source])\}$\;
        Initialize $N$ $mutex\_threads$\;
        Initialize $V$ $mutex\_nodes$\;
        Launch $N$ threads\;
        Join $N$ threads\;
        Reconstruct final path\;
        \KwRet\;
}
\SetKwFunction{FHDA}{HDA}
  \SetKwProg{Fn}{Function}{:}{}
  \Fn{\FHDA{$index$, $dest$, $N$, $heuristic$}}{
    \While{$1$}{
      \While{$!openSet[index].EMPTY()$}{
        LOCK($mutex\_threads[index]$)
        $a \gets openSet.POP()$\;
        UNLOCK($mutex\_threads[index]$)
      }
      \If($a$ is duplicate){
        continue\;
      }
      \ForEach{neighbor $b$ of $a$}{
        $wt \gets weight(a, b)$\;
        $tentativeScore \gets g[a] + wt$\;
        \If{$tentativeScore$ is less than $g[b]$}{
          $owner\_a \gets hash(a,N)$\;
          $owner\_b \gets hash(b,N)$\;
          \If($b$ is duplicate){
            continue\;
          }
          LOCK($mutex\_nodes[a]$)
          $tentativeScore \gets g[a] + wt$\;
          UNLOCK($mutex\_nodes[a]$)

          LOCK($mutex\_nodes[b]$)
          \If{$tentativeScore$ is less than $g[b]$}{
            $parentVertex[b] \gets a$\;
            $costToCome[b] \gets wt$\;
            $g[b] \gets tentativeScore$\;
            $f[b] \gets g[b] + h[b]$\;
            LOCK($mutex\_threads[owner\_b]$)
            $f[owner\_b][b] \gets f[b]$\;
            $openSet.PUSH((b, f[b]))$\;
            UNLOCK($mutex\_threads[owner\_b]$)
          }
          UNLOCK($mutex\_nodes[a]$)
        }
      }

      \If{$openSet[index].EMPTY()$ and $parentVertex[b] != -1$}{
        Terminate according to B or SF\;
      }
    }
        \KwRet\;
}
\end{algorithm*}
\end{comment}
\subsection{Results}
(Plots with the comparison of all the models with sequential A* on
random, BAY, FLA)

\subsection{Parallel Bidirectional Search}
(Explanation + Pseudocode)
\subsubsection{Results}
(Plots to show thread 1 and 2 works, plots of time)

\section{Complete Results}
(Tables with numbers)

\section{Final Considerations}
(Comments)

\section{DIMACS Benchmark}
(More detailed explanation of the input format of the benchmarks)

\section{Future Works}
(Possible improvements)




%%%%%%%%%%%% Supplementary Methods %%%%%%%%%%%%
%\footnotesize
%\section*{Methods}

%%%%%%%%%%%%% Acknowledgements %%%%%%%%%%%%%
%\footnotesize
%\section*{Acknowledgements}

%%%%%%%%%%%%%%   Bibliography   %%%%%%%%%%%%%%
\normalsize
\bibliography{references}

%%%%%%%%%%%%  Supplementary Figures  %%%%%%%%%%%%
%\clearpage

%%%%%%%%%%%%%%%%   End   %%%%%%%%%%%%%%%%
%\end{multicols}  % Method B for two-column formatting (doesn't play well with line numbers), comment out if using method A
\end{document}