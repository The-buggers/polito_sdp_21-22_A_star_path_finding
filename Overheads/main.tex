\documentclass[12pt]{beamer}
%\usepackage{amsmath, comment, amsthm, amssymb, amsfonts, multicol,  graphicx, caption, subcaption, multirow}
\usepackage{algpseudocode}
\usepackage{hyperref}
\graphicspath{ {../Report/Pictures/} }
\usetheme{Boadilla}
\title{Parallel A* Project}
\subtitle{System And Device Programming}
\author{Lorenzo Ippolito, Fabio Mirto, Mattia Rosso}
\institute{Politecnico di Torino}
\date{\today}
\AtBeginSection[]{
    \begin{frame}<beamer>
        \frametitle{Outline}
        \begin{columns}[T]
            \begin{column}{.45\textwidth}
                \tableofcontents[currentsection,sections=1-6,hideallsubsections]
            \end{column}
            \begin{column}{.45\textwidth}
                \tableofcontents[currentsection,sections=7-11,hideallsubsections]
            \end{column}
        \end{columns}
    \end{frame}
}
%\usetheme{lucid}
\begin{document}
	\begin{frame}
		\titlepage
	\end{frame}
	\begin{frame}[allowframebreaks]{Outline}
		\tableofcontents
	\end{frame}
	%% %%%%%%%%%%%%%%%%%%%%%%%%%% %%
	%% CHAPTER 1: A* INTRODUCTION %%
	%% %%%%%%%%%%%%%%%%%%%%%%%%%% %%
	\section{Introduction: about the A* algorithm}
	\subsection{Problem definition}
	\begin{frame}{\secname}
		\framesubtitle{\subsecname}
		\begin{itemize}
			\item A* is a graph-traversal and path-search algorithm used in many contexts of computer science and 
			      not only
			\item It can be considered as a general case of the Dijkstra algorithm
			\item It is a Greedy-best-first-search algorithm that uses an heuristic function to guide
				  itself
		\end{itemize}
	\end{frame}
	\begin{frame}{\secname}
		\framesubtitle{\subsecname}
		What it does is combining:
		\begin{itemize}
			\item \textbf{Dijkstra approach}: favoring nodes closed to the starting point(source)
			\item \textbf{Greedy-best-first-search approach}: favoring nodes closed to the final point(destination)
		\end{itemize}
	\end{frame}
	\begin{frame}{\secname}
		\framesubtitle{\subsecname}
		According to the standard terminology:
		\begin{itemize}
			\item $g(n)$: exact cost of moving from source to n
			\item $h(n)$: heuristic estimated cost of moving from a node n(source included) to the destination
			\item $f(n) = g(n) + h(n)$: in this way we are able to combine the actual cost with the estimated one
		\end{itemize}
		At each iteration the node $n$ that has the minimum $f(n)$ is examinated(expanded).
	\end{frame}
	\subsection{Heuristic design}
	\begin{frame}{\secname}
		\framesubtitle{\subsecname: properties}
		The heurstic function represents the acutal core of the A* algorithm. It is a prior-knowledge that
		we have about the cost of the path from every node (source included) to the destination. The properties
		of an heuristic function are:
		\begin{block}{Heuristic function properties}
			\begin{itemize}
			\item 	Admissibility: $h(n) < cost(n, dest) \;\forall n \in V$
			\item 	Consistency: $h(x) \le cost(x, y) + h(y) \; \forall (x, y) \in E$  
			\end{itemize}
		\end{block}
	\end{frame}
	\begin{frame}{\secname}
		\framesubtitle{\subsecname: corner cases}
		Three relevant situations are:
		\begin{itemize}
			\item \textbf{Dijkstra}: if $h(n)=0$ for every node in the graph.
			\item \textbf{Ideal heuristic}: if $h(n)$ is exactly equal to the cost of moving from $n$ to
				  the destination.
			\item \textbf{Full greedy-best-first search}: if $h(n) \gg g(n)$ than only $h(n)$ plays a role.
		\end{itemize}
	\end{frame}
	%% %%%%%%%%%%%%%%%%%%%%%%%%% %%
	%% CHAPTER 2: A* APPLICATION %%
	%% %%%%%%%%%%%%%%%%%%%%%%%%% %%
	\section{A* project application}
	\subsection{Geographical pathfinding}
	\begin{frame}{\secname}
		\framesubtitle{\subsecname}
		We will work with a weighted oriented graph $G$ that is made of nodes $n \in V$ that represents
		road-realated points of interest and edges $(x,y) \in E$ that represent unidirectional connections between these points.
		Each edge $(x, y)$ is associated to a weight that is the great-circle distance between $x$ and $y$ measured
		in meters.
	\end{frame}
	\subsection{Problem-specific heuristic function}
	\begin{frame}{\secname}
		\frametitle{2. A* project application}
		\framesubtitle{The great-circle distance}
		We will apply the Haversine formula to compute the distance from node $(\phi_1,\lambda_1)$
		to node $(\phi_1,\lambda_1)$ where $\phi$ is the latitude and $\lambda$ is the longitude:
		
		\begin{block}{Haversine Formula}
			\begin{center}
				$d = R \cdot c$\\
				$c = 2 \cdot atan2(\sqrt{a},\sqrt{1-a})$\\
				$a = sin^2\Big({\frac{\Delta \phi}{2}}\Big) + cos(\phi_1) \cdot cos(\phi_2) \cdot sin^2\Big({\frac{\Delta \lambda}{2}}\Big)$
				\\$R=6.371km$
			\end{center}
		\end{block}
	\end{frame}
	%% %%%%%%%%%%%%%%%%%%%%%% %%
	%% CHAPTER 3: INPUT GRAPH %%
	%% %%%%%%%%%%%%%%%%%%%%%% %%
	\section{Graph file structure}
	\subsection{File input format}
	\begin{frame}{\secname}
		\framesubtitle{\subsecname}
		The format the file we have worked with is:
		\begin{itemize}
			\item First line: the number of nodes $N[int]$
			\item N following lines: nodes appearing as $(index[int], longitude[double], latidue[double])$
			\item E following lines (with E unknown): edges appearing as $(x[int], y[int], weight[double])$
		\end{itemize}
	\end{frame}
	\subsection{DIMACS benchmark}
	\begin{frame}{\secname}
		\framesubtitle{\subsecname}
		The benchmark files we have used come from the 
		%\href{http://www.diag.uniroma1.it//challenge9/download.shtml#benchmark}{DIMACS benchmark}. 
		Here each geographic map is described by:
		\begin{itemize}
			\item \textit{.co} file: a file containing the coordinates of the nodes following the FIPS system notation
			\item \textit{.gr} file: a file containing the edges and the relative weight(distance) expressed in meters
		\end{itemize}
		We have properly converted these files to obtain binary files with the previously described format
	\end{frame}
	\subsection{Selected benchmark paths}
	\begin{frame}{\secname}
		\framesubtitle{\subsecname}
		\begin{figure}[ht!]
			\centering
			\includegraphics[width=0.65\linewidth]{others/google_maps.png}
			\caption{From left to right top to bottom BAY, FLA, W, USA}
			\label{testpaths}
		\end{figure}
	\end{frame}
	%% %%%%%%%%%%%%%%%%%%%%%%%% %%
	%% CHAPTER 4: SEQUENTIAL A* %%
	%% %%%%%%%%%%%%%%%%%%%%%%%% %%
	\section{A* sequential algorithm}
	\subsection{Data structures}
	\begin{frame}{\secname}
		\framesubtitle{\subsecname}
		%\framesubtitle{Pseudocode}
		The first step consists of a pre-computation of:
		\begin{itemize}
		\item The heuristic $h(n)$ for each node computed through the Haversine formula.
		\item The initial values of $f(n)$ and $g(n)$ that will be set to $DOUBLE\_MAX$ for each
				node except for the source node that will have $f(source) = h(source)$ and $g(source) = 0$.
		\end{itemize}
		Relevant data structures are:
		\begin{itemize}
			\item \textit{costToCome} \texttt{costToCome[i]} contains the current
					best cost to reach node \texttt{i}
			\item \textit{parentVertex}: \texttt{parentVertex[i]} contains the
					parent node of node \texttt{i} according to the current best path found to reach
					the destination
			\item \textit{openSet}: nodes that have to explored (implemented as a Prioriy Queue with
				   $f(n)$ as prioriry)
			\item \textit{closedSet}: already explored nodes 
		\end{itemize}
	\end{frame}
	\subsection{Pseudocode}
	\begin{frame}[allowframebreaks]{\secname}
		\framesubtitle{\subsecname}
		\begin{algorithmic}[1]
			\Function{$astar$}{$G, source, dest, h$}
			\State $g[i] \gets DOUBLE\_MAX \;\forall i \in V$\;
			\State $f[i] \gets DOUBLE\_MAX \;\forall i \in V$\;
			\State $h[i] \gets h(i, d) \; \forall i \in V$\;
			\State $parentVertex[i] \gets -1 \; \forall i \in V$\;
			\State $f[source] \gets h[source]$\;
			\State $g[source] \gets 0$\;
			\State $openSet := \{(source, f[source])\}$\;
			\While{$!openSet.EMPTY()$}
				\State $a \gets openSet.POP()$\;
			\If{$a == dest$}
				\State reconstructPath()\;
				\State return\;
			\EndIf
			\If{$a \in closedSed$}
				\State continue\;
			\EndIf
			\State $closedSed.PUSH(a)$
			\ForAll{neighbors $b$ of $a$}
				\If{$b \in closedSed$}
					\State continue\;
				\EndIf
				\State $wt \gets weight(a, b)$\;
				\State $tentativeScore \gets g[a] + wt$\;
				\If{$tentativeScore < g[b]$}
					\State $parentVertex[b] \gets a$\;
					\State $costToCome[b] \gets wt$\;
					\State $g[b] \gets tentativeScore$\;
					\State $f[b] \gets g[b] + h[b]$\;
					\State $openSet.PUSH((b, f[b]))$\;
				\EndIf
			\EndFor
			\EndWhile
			\EndFunction
		  \end{algorithmic}
	\end{frame}

	\begin{frame}
		\frametitle{4. Sequential A* Algorithm}
		\framesubtitle{Results}
		\begin{table}[ht!]
			\caption{Sequential reading + Sequential A* performance}
			\label{seqtimes}
			\begin{tabular}{|l|l|l|l|l|l|}
			\hline
			\textbf{}    & \textbf{File Size} & \textbf{Reading} & \textbf{A*} & \textbf{Total} & \textbf{\begin{tabular}[c]{@{}l@{}}Reading \\ Impact\end{tabular}} \\ \hline
			\textbf{BAY} & 20.51MB              & 0.9538s           & 0.2197s      & 1.1735s          & 81.3\%                                                         \\ \hline
			\textbf{FLA} & 69.09MB              & 3.1551s           & 0.7174s      & 3.8725s          & 81.5\%                                                           \\ \hline
			\textbf{W}   & 394.26MB             & 18.3065s          & 2.5890s      & 20.8955s         & 87.6\%                                                           \\ \hline
			\textbf{USA} & 1292.40MB            & 56.9942s          & 13.6716s     & 70.6658s         & 80.6\%                                                           \\ \hline
			\end{tabular}
		  \end{table}
		The reading phase has an high impact on the total execution time
	\end{frame}
	%% %%%%%%%%%%%%%%%%%%%%%%%%% %%
	%% CHAPTER 5: A* vs DIJKSTRA %%
	%% %%%%%%%%%%%%%%%%%%%%%%%%% %%
	\section{A* and Dijkstra}
	\subsection{Expanded nodes}
	\begin{frame}{\secname}
		\framesubtitle{\subsecname}
		\begin{table}[ht!]
			\caption{Expanded nodes in different maps}
			\begin{tabular}{|l|l|l|}
			\hline
			\textbf{} & \textbf{Dijkstra} & \textbf{Sequential A*}       \\ \hline
			\textbf{BAY}            & 318725 of 321270    & 157137 of 321270  \\ \hline
			\textbf{FLA}            & 996956 of 1070376   & 592480 of 1070376 \\ \hline
			\textbf{W}              & 5470394 of 1070376  & 1600083 of 1070376 \\ \hline
			\textbf{USA}            & 16676528 of 1070376 & 8998767 of 1070376 \\ \hline
			\end{tabular}
		\end{table}
	\end{frame}
	\begin{frame}{\secname}
		\framesubtitle{\subsecname}
		\begin{figure}[ht!]
			\centering
			\includegraphics[width=0.8\linewidth]{astar_dijkstra/expanded_nodes.png}
			\caption{Expanded nodes: A* vs Dijkstra}
			\label{histogramnodes}
		\end{figure}
		TODO We can notice that...
	\end{frame}
	\begin{frame}{\secname}
		\framesubtitle{\subsecname}
		\begin{figure}[ht!]
			\centering
			\includegraphics[width=0.8\linewidth]{astar_dijkstra/dijkstra_astar.png}
			\caption{Test paths on BAY(left) and FLA(right)}
			\label{astardijkstra}
		\end{figure}
	\end{frame}
	\subsection{Time and resources comparison}
	\begin{frame}{\secname}
		\framesubtitle{\subsecname}
		\begin{figure}[ht!]
			\centering
			\includegraphics[width=0.8\linewidth]{astar_dijkstra/execution_time.png}
			\caption{Execution time: A* vs Dijkstra}
			\label{astardijkstratime}
		  \end{figure}
	\end{frame}
	\begin{frame}{\secname}
		\framesubtitle{\subsecname}
		\begin{figure}[ht!]
			\centering
			\includegraphics[width=0.8\linewidth]{astar_dijkstra/cpumem.png}
			\caption{Exploited resources: A* vs Dijkstra}
			\label{astardijkstracpumem}
		\end{figure}
	\end{frame}
	%% %%%%%%%%%%%%%%%%%%%%%%%% %%
	%% CHAPTER 6: PARALLEL READ %%
	%% %%%%%%%%%%%%%%%%%%%%%%%% %%
	\section{Parallel input file reading}
	\subsection{Approach 1 (RA1)}
	\begin{frame}{\secname}
		\framesubtitle{\subsecname}
		This reading approach is based on:
		\begin{itemize}
			\item Memory mapping of the input file
			\item Concurrent reading of the memory mapped data structure
		\end{itemize}
		The reading of the input file in a parallel way happens using $N$ threads that concurrently access
		the pointer to the memory mapped file. While the reading of the input file can happen in whatever order
		and concurrency doesn't represent a problem the loading of Graph data structures must happen in
		mutual exclusion and this is why two locks are used:
		\begin{itemize}
			\item One lock to protect the loading of the nodes in the Graph (to fill the Symbol Table)
			\item One lock to protect the loading of the edges in the Linked List data structure of the Graph
		\end{itemize}
	\end{frame}
	\subsection{Results on FLA map}
	\begin{frame}{\secname}
		\framesubtitle{\subsecname}
		\begin{figure}[ht!]
			\centering
			\includegraphics[width=0.85\linewidth]{read/par_read_1_time.png}
			\caption{Performance of RA1 for different number of threads}
			\label{parread1time}
	 	 \end{figure}	
	\end{frame}
	\subsection{Approach 2 (RA2)}
	\begin{frame}{\secname}
		\framesubtitle{\subsecname}
		This is not properly a different approach of parallel reading but simply a version
		of the \textit{RA1} needed when the \textit{PNBA*} algorithm will be applied.
		Since we are loading two graphs data structures at the same time the performances will
		be compared with the sequential reading as if it was run twice (the first time to read $G$ and
		the second time to read $R$)
	\end{frame}
	\subsection{Results on FLA map}
	\begin{frame}{\secname}
		\framesubtitle{\subsecname}
		\begin{figure}[ht!]
			\centering
			\includegraphics[width=0.8\linewidth]{read/par_read_2_time.png}
			%\caption{Performance of approach 2 for different number of threads}
			\label{parread2time}
		  \end{figure}
	\end{frame}
	\subsection{RA1 and RA2 on FLA map}
	\begin{frame}{\secname}
		\framesubtitle{\subsecname}
		\begin{figure}[ht!]
			\centering
			\includegraphics[width=1\linewidth]{read/par_read_all_time.png}
			\caption{RA(Read Approach) 1,2,3 compared with sequential reading}
			\label{parreadalltime}
		\end{figure}
	\end{frame}
	\subsection{Results on all maps}
	\begin{frame}{\secname}
		\framesubtitle{\subsecname}
		\begin{table}[ht!]
			\centering
			  \caption{RA1 results against sequential reading}
			  \begin{tabular}{|l|l|l|l|}
			  \hline
			  \textbf{}    & \textbf{\begin{tabular}[c]{@{}l@{}}RA1\\ (2 threads)\end{tabular}} & \textbf{Sequential} & \textbf{Speed-Up} \\ \hline
			  \textbf{BAY} & 0.1936s                                                            & 0.9366s             & 79.3\%            \\ \hline
			  \textbf{FLA} & 0.5103s                                                            & 3.0650s             & 83.4\%            \\ \hline
			  \textbf{W} & 3.3303s                                                              & 17.8834s            & 81.4\%            \\ \hline
			  \textbf{USA} & 14.1492s                                                           & 56.4445s            & 74.9\%            \\ \hline
			  \end{tabular}
			  \label{readresultsfinal}
		  \end{table}
	\end{frame}
	\begin{frame}{\secname}
		\framesubtitle{\subsecname}
		\begin{table}[ht!]
			\centering
			  \caption{RA2 results against sequential reading}
			  \begin{tabular}{|l|l|l|l|}
			  \hline
			  \textbf{}    & \textbf{\begin{tabular}[c]{@{}l@{}}RA2\\ (2 threads)\end{tabular}} & \textbf{Sequential(x2)} & \textbf{Speed-Up} \\ \hline
			  \textbf{BAY} & 0.5313s                                                           & 1.8732s              & 71.6\%            \\ \hline
			  \textbf{FLA} & 1.4682s                                                            & 6.1300s             & 76.1\%            \\ \hline
			  \textbf{W} & 10.4959s                                                             & 35.7668s            & 70.7\%            \\ \hline
			  \textbf{USA} & 35.4505s                                                           & 112.8890s            & 68.6\%            \\ \hline
			  \end{tabular}
			  \label{readresultsfinal}
		  \end{table}
	\end{frame}
	\begin{frame}{\secname}
		\framesubtitle{\subsecname}
		\begin{figure}[ht!]
			\centering
			\includegraphics[width=1\linewidth]{read/parread_cpumem.png}
			\caption{RA(Read Approach) 1,2 compared with sequential reading on FLA - exploited resources}
			\label{parreadallcpumem}
		  \end{figure}
	\end{frame}
	%% %%%%%%%%%%%%%%%%%%%%%% %%
	%% CHAPTER 7: PARALLEL A* %%
	%% %%%%%%%%%%%%%%%%%%%%%% %%
	\section{Parallel A*}
	\begin{frame}{\secname}
		The goal of the project was to find one or more parallel versions of the A* algorithm 
		and showing their performances w.r.t. the sequential version. We have followed two approaches
		to face this problem:: 
		\begin{itemize}
			\item \textbf{Hash Distributed A* (HDA*)}: it puts in action a complex way of parallelizing A* by defining a hash-based work distribution strategy.
		    \item \textbf{Parallel New Bidirectional A* (PNBA*)}: parallel search of the path from \textit{source} to \textit{dest}
				  and of the path from \textit{dest} to \textit{source} in the reversed graph.
		\end{itemize}
	\end{frame}
	\subsection{HDA*}
	\begin{frame}{\secname : \subsecname}
		%\framesubtitle{\subsecname}
		\begin{itemize}
			\item \textit{Ownership}: HDA* work is based on the fact that each thread is \textit{owner} of a
				  specific set of nodes of the Graph -> given a node $n$ it is defined a hash function $f:\;f(n) = t$
				  where $t \in \{1..N\}$ with $N$ the number of threads
			\item When a thread extracts from the \textit{open set}
			      (expands) a node all its neighbors are added to the \textit{open set} of the owner thread of the expanded node.
		\end{itemize}
	\end{frame}
	\begin{frame}{\secname : \subsecname}
		%\framesubtitle{\subsecname} 
		HDA* doesn't provide the same guarantees of the sequential algorithm:
		\begin{itemize}
		\item   In sequential A* if it's provided an heuristic function that is both \textit{admissible}
				and \textit{consistent} we have the guarantee that each node will be only expanded once
				and that the first time we expand that node we have found a shortest path to it.
		\item   In HDA* we loose these guarantees: since we don't know in which order nodes will be processed
				it could happen that a longer path to \textit{dest} is found before the shortest one so
				a node could be opened more than once and expanding the \textit{dest} node doesn't
				mean that we have terminated.
		\end{itemize}
	\end{frame}
	\subsubsection{The hash function}
	\begin{frame}{\secname : \subsecname}
		\framesubtitle{The hash function} 
		We have implemented two types of hash functions:
		\begin{block}{Hash functions definitions}
			$hash_1(node\_index, num\_threads) = node\_index \;\%\; num\_threads$
			$hash_2(node\_index, num\_threads, V) = i-1 \;with\;
			i = min_i : \frac{V}{num\_threads}\cdot i > node\_index \;, i \in \{1,...,num\_threads\}$
		\end{block}
		\begin{itemize}
			\item The first one simply assigns a node to a thread in a randomic fashion w.r.t to its position
				  inside the map
			\item What tries to do the second hash function (the one that we have used to 
				  measure performances) is simply assigning nodes to threads following their index
				  numbering (e.g. nodes from $0$ to $K$ to thread $t_0$, nodes from
				  $K+1$ to $H$ to thread $t_1$ and so on and so forth).
		\end{itemize}
	\end{frame}
	\subsubsection{Work distribution on BAY map}
	\begin{frame}{\secname : \subsecname}
		\framesubtitle{Work distribution on BAY map}
		\begin{figure}[ht!]
			\centering
			\small
			\includegraphics[width=0.8\linewidth]{hda/hda_work_BAY.png}
			\caption{HDA* work distribution among 3 threads in BAY map}
			\label{hdawork1}
		  \end{figure}
	\end{frame}
	\subsubsection{Distributed termination condition}
	\begin{frame}{\secname : \subsecname}
		\framesubtitle{Distributed termination condition}
		\begin{itemize}
			\item \textbf{Barrier method (B)}: when a thread realizes that its \textit{open set} is
			empty a barrier is 
			hitten and when all the threads have hitten the barrier
			each one makes a check to confirm(or not) that all the \textit{open sets} of all
			the threads are still empty. If this is not true it means that there are nodes
			that have still to be processed and the best path to $dest$ found so far could
			not be the optimal one otherwise all the threads can terminate.
			\item \textbf{Sum-Flag method (SF)}: the idea behind the sum flag method comes from the fact that
			the Barrier mechanism could be quite expensive. In this termination condition method
			each thread keeps a binary flag saying whether its \textit{open set} is empty
			or not. When no more nodes are inside it the flag is set and if $\sum_{i=1}^{N}flag[i] = N$
			all the threads can correctly terminate.
		\end{itemize}
	\end{frame}
	\subsubsection{Communication methodology}
	\begin{frame}{\secname : \subsecname}
		\framesubtitle{Communication methodology}
		\begin{itemize}
			\item \textbf{Shared Address Space (SAS)}: threads share pointers to common data structures (we need
			to cope with mutual exclusion)
			\item \textbf{Message Passing (MP)}: threads can communicate only through messages via:
			\begin{itemize}
				\item Message Queues
				\item Shared Memory
			\end{itemize}
		\end{itemize}
	\end{frame}
	\subsubsection{Message Passing Model (MP)}
	\begin{frame}{\secname : \subsecname}
		\framesubtitle{MP using Shared Memory (MPSM)}
		We don't report here all the details regarding Message Passing implemented using Linux Message Queues
		because of its lack of scalability. Aobut MPSM:
		\begin{itemize}
			\item Some data structures are shared: $parentVertex$, $costToCome$
			\item The exchange of information about discovered nodes that have to be explored is done
				  using Linux Shared Memory.
			\item This has the great advantage of minimizing the resource contention among threads 
			      (something that will strongly penalize the SAS model).
			\item The termination condition that was implemented is Sum-Flag(SF)
		\end{itemize}
	\end{frame}
	\begin{frame}{\secname : \subsecname}
		\framesubtitle{MP using Shared Memory (MPSM)}
		Each message sent from $t_i$ to $t_j$ regarding node $n$ contains:
		\begin{itemize}
			\item The index of the node $n$
			\item The parent node of $n$ (the one expanded from the \textit{open set} of $t_i$)
			\item The weight of the edge $(a, n)$ where $a$ is the parent node of $n$
			\item The value of $g[n]$ according to thread $t_i$
		\end{itemize}
		The way how messages are read and written by the different threads is done adopting the \textit{Readers \& Writers}
		model where:
		\begin{itemize}
		\item Thread writer $t_i$ writes on Shared Memory increasing a global pointer $smp_G$.
		\item Thread reader $t_j$ reads from the Shared Memory by increasing a local pointer $smp_j$. The reading
				of messages terminates when $smp_j == smp_G$.
		\end{itemize}
	\end{frame}
	\begin{frame}{\secname : \subsecname}
		\framesubtitle{MP using Shared Memory (MPSM)}
		The termination condition we have implemented is Sum-Flag method.
		\begin{itemize}
			\item A $globalDestCost$ variable is combined with the distributed
				  termination condition in order to know wheter threads can correctly stop (an
				  alternative was to check wheter $parentVertex[dest]==-1$).
			\item \textit{Readers \& Writers} mechanism was iplemented by using Linux Semaphores.
		\end{itemize}
	\end{frame}
	\begin{frame}[allowframebreaks]{\secname : \subsecname}
		\framesubtitle{MPSM pseudocode}
		\begin{algorithmic}[1]
			\Function{$hda\_mp\_sm$}{}
				\State $smp_L \gets smp_G$\;
				\While{$1$}
					\While{$smp_L \neq  smp_G$}
						\State $wait(meR)$\;
						\State $nR++$\;
						\If{$nR == 1$}
							\State $wait(w)$\;
						\EndIf
						\State $post(meR)$\;
						\If{$hash_2(index, N, V) == index$}
							\If{$smp_L.g < g[smp_L.n]$}
								\State $g[smp_L.n] \gets smp_L.g$\;
								\State $f[smp_L.n] \gets g[smp_L.n]+h[smp_L.n]$\;
								\State $parentVertex[smp_L.n] \gets smp_L.previous$\;
								\State $costToCome[smp_L.n] \gets smp_L.wt$\;
								\State $openSet.PUSH(\{f[smp_L.n], smp_L.n\}$)\;
							\EndIf
						\EndIf
						\State $smp_L++$\;
						\State $wait(meR)$\;
						\State $nR--$\;
						\If{$nR == 0$}
							\State $post(w)$\;
						\EndIf
						\State $post(meR)$\;
					\EndWhile
					\If{$openSet.EMPTY()$ and $globalDestCost < \infty$}
        				\State Terminate according to SF\;
      				\EndIf
					\If{$!openSet.EMPTY()$}
						\State $a \gets openSet.POP()$\;
						\If{$a == dest$}
							\State $LOCK$\;
							\State $globalDestCost \gets g[a]$
							\State $UNLOCK$\;
						\EndIf
						\For{neighbor $b$ of $a$}
							\State $owner\_b \gets hash_2(b, N, V)$\;
							\If{$owner\_b  ==  index$}
								\State $parentVertex[b] \gets a$\;
								\State $costToCome[b] \gets weight(a, b)$\;
								\State $g[b] \gets g[b] + weight(a, b)$\;
								\State $f[b] \gets g[b] + h[b]$\;
								\State $openSet.PUSH((b, f[owner\_b][b]))$\;
							\Else
								\State $wait(w)$
								\State $smp_G.n \gets b$\;
								\State $smp_G.g \gets g[b]$\;
								\State $smp_G.wt \gets weight(a, b)$\;
								\State $smp_G.previous \gets a$\;
								\State $smp_G++$\;
								\State $post(w)$
							\EndIf
						\EndFor
					\EndIf
				\EndWhile
		  \EndFunction
		  \end{algorithmic}
	\end{frame}
	\subsubsection{Shared Address Space (SAS)}
	\begin{frame}{\secname : \subsecname}
		\framesubtitle{Shared Address Space (SAS)}
		The shared data structures are:
		\begin{itemize}
			\item A global array of \textit{open sets} of size $N$ (number of threads)
				  where $openSet[i]$ contains a pointer to the \textit{open set} of thread $t_i$.
			\item The $parentVertex$ and $costToCome$ data structures are shared among all the threads.
		\end{itemize}
		This approach clearly requires locks so that the operations on the shared data structures
		can happen in mutual exclusion. In particular we need:
		\begin{itemize}
			\item $N$ $mutex\_threads$ locks (one for each \textit{open set} so $N$ overall).
			\item $V$ $mutex\_nodes$ locks (one for each node of the graph)
		\end{itemize}
	\end{frame}
	\subsubsection{SAS pseudocode}
	\begin{frame}[allowframebreaks]{\secname : \subsecname}
		\framesubtitle{SAS pseudocode}
		\begin{algorithmic}[1]
			\Function{$hda\_sas$}{}
				\While{$1$}
      				\While{$!openSet[index].EMPTY()$}
        				\State $LOCK(mutex\_threads[index])$\;
        				\State $a \gets openSet.POP()$\;
        				\State $UNLOCK(mutex\_threads[index])$\;
      				\EndWhile
      			\If{$a$ is duplicate}
        			\State continue\;
      			\EndIf
      		    \For{neighbor $b$ of $a$}
        			\State $wt \gets weight(a, b)$\;
        			\State $tentativeScore \gets g[a] + wt$\;
        			\If{$tentativeScore < g[b]$}
          				\State $owner\_a \gets hash_2(a,N,V)$\;
          				\State $owner\_b \gets hash_2(b,N,V)$\;
					\EndIf
          			\If{$b$ is duplicate}
            			\State continue\;
          			\EndIf
          			\State $tentativeScore \gets g[a] + wt$\;

          			\State LOCK($mutex\_nodes[b]$)\;
          			\If{$tentativeScore$ is less than $g[b]$}
					    \State $parentVertex[b] \gets a$\;
            			\State $costToCome[b] \gets wt$\;
            			\State $g[b] \gets tentativeScore$\;
            			\State $f[b] \gets g[b] + h[b]$\;
            			\State $LOCK(mutex\_threads[owner\_b])$\;
            			\State $f[owner\_b][b] \gets f[b]$\;
            			\State $openSet.PUSH(\{b, f[owner\_b][b]\})$\;
            			\State $UNLOCK(mutex\_threads[owner\_b])$\;
          			\EndIf
          			\State $UNLOCK(mutex\_nodes[b])$\;
        		\EndFor
				\EndWhile

      		\If{$openSet[index].EMPTY()$ and $parentVertex[dest] \neq -1$}
        		\State Terminate according to B or SF\;
      		\EndIf
		  \EndFunction
		  \end{algorithmic}
	\end{frame}
	\subsubsection{Results}
	\begin{frame}{\secname : \subsecname}
		\framesubtitle{Results}
		\begin{figure}[ht!]
			\centering
			\includegraphics[width=0.8\linewidth]{hda/baytime.png}
			\caption{HDA* on BAY map - time}
		\end{figure}
	\end{frame}
	\begin{frame}{\secname : \subsecname}
		\framesubtitle{Results}
		\begin{figure}[ht!]
			\centering
			\includegraphics[width=0.8\linewidth]{hda/baycpumem.png}
			\caption{HDA* on BAY map - resources}
		\end{figure}
	\end{frame}

	\begin{frame}{\secname : \subsecname}
		\framesubtitle{Results}
		\begin{figure}[ht!]
			\centering
			\includegraphics[width=0.8\linewidth]{hda/flatime.png}
			\caption{HDA* on FLA map - time}
		\end{figure}
	\end{frame}
	\begin{frame}{\secname : \subsecname}
		\framesubtitle{Results}
		\begin{figure}[ht!]
			\centering
			\includegraphics[width=0.8\linewidth]{hda/flacpumem.png}
			\caption{HDA* on FLA map - resources}
		\end{figure}
	\end{frame}

	\begin{frame}{\secname : \subsecname}
		\framesubtitle{Results}
		\begin{figure}[ht!]
			\centering
			\includegraphics[width=0.8\linewidth]{hda/wtime.png}
			\caption{HDA* on W map - time}
		\end{figure}
	\end{frame}
	\begin{frame}{\secname : \subsecname}
		\framesubtitle{Results}
		\begin{figure}[ht!]
			\centering
			\includegraphics[width=0.8\linewidth]{hda/wcpumem.png}
			\caption{HDA* on W map - resources}
		\end{figure}
	\end{frame}

	\begin{frame}{\secname : \subsecname}
		\framesubtitle{Results}
		\begin{figure}[ht!]
			\centering
			\includegraphics[width=0.8\linewidth]{hda/usatime.png}
			\caption{HDA* on USA map - time}
		\end{figure}
	\end{frame}
	\begin{frame}{\secname : \subsecname}
		\framesubtitle{Results}
		\begin{figure}[ht!]
			\centering
			\includegraphics[width=0.8\linewidth]{hda/usacpumem.png}
			\caption{HDA* on USA map - resources}
		\end{figure}
	\end{frame}

	\begin{frame}{\secname : \subsecname}
		\framesubtitle{Results}
		\begin{figure}[ht!]
			\centering
			\includegraphics[width=0.8\linewidth]{hda/seq_sf_time.png}
			\caption{HDA* SAS-SF vs Sequential A* on all maps - time}
		\end{figure}
	\end{frame}
	\begin{frame}{\secname : \subsecname}
		\framesubtitle{Results}
		\begin{figure}[ht!]
			\centering
			\includegraphics[width=0.8\linewidth]{hda/seq_sf_cpumem.png}
			\caption{HDA* SAS-SF vs Sequential A* on all maps - resources }
		\end{figure}
	\end{frame}

	\begin{frame}{\secname : \subsecname}
		\framesubtitle{Results}
		\begin{figure}[ht!]
			\centering
			\includegraphics[width=0.8\linewidth]{hda/seq_mpsm_time.png}
			\caption{HDA* MP-SM vs Sequential A* on all maps - time}
		\end{figure}
	\end{frame}
	\begin{frame}{\secname : \subsecname}
		\framesubtitle{Results}
		\begin{figure}[ht!]
			\centering
			\includegraphics[width=0.8\linewidth]{hda/seq_mpsm_cpumem.png}
			\caption{HDA* MP-SM vs Sequential A* on all maps - resources}
		\end{figure}
	\end{frame}
	\subsubsection{Comments and overall results}
	\begin{frame}{\secname : \subsecname}
		\framesubtitle{Results}
		\begin{table}[ht!]
			\centering
			\caption{SAS-SF with best number of threads time performances}
			\begin{tabular}{|l|l|l|l|l|}
			\hline
			\textbf{}    & \textbf{Threads} & \textbf{SAS-SF} & \textbf{Sequential A*} & \textbf{Perf.}\\ \hline
			\textbf{BAY} & 5        & 0.5097s                & 0.2647s  &-92.6\%          \\ \hline
			\textbf{FLA} & 7        & 1.6383s                & 0.7174s  &-128.3\%          \\ \hline
			\textbf{W}   & 7        & 10.8626s                & 2.5890s &-319.6\%           \\ \hline
			\textbf{USA} & 7         & 30.5655s               & 13.6716 &-123.6\%           \\ \hline
			\end{tabular}
			\label{tablesas}
		  \end{table}
	\end{frame}
	\begin{frame}{\secname : \subsecname}
		\framesubtitle{Results}
		\begin{table}[ht!]
			\centering
			\caption{MP-SM with best number of threads time performances}
			\begin{tabular}{|l|l|l|l|l|}
			\hline
			\textbf{}    & \textbf{Threads} & \textbf{MPSM} & \textbf{Sequential A*} & \textbf{Perf.}\\ \hline
			\textbf{BAY} & 5        & 0.1770s                & 0.2647s  &+33.1\%          \\ \hline
			\textbf{FLA} & 3        & 0.4922s                & 0.7174s  &+31.4\%          \\ \hline
			\textbf{W}   & 2        & 11.1136s                & 2.5890s &-329.3\%           \\ \hline
			\textbf{USA} & -         & -                      & 13.6716 &-           \\ \hline
			\end{tabular}
			\label{tablempsm}
		  \end{table}
	\end{frame}
	\begin{frame}[allowframebreaks]{\secname : \subsecname}
		\framesubtitle{Comments and overall results}
		\begin{itemize}
			\item The MP-MQ variant of HDA* is not scalable.
	  		\item Better performances are achieved by the MP-SM variant. On BAY and FLA it achieves very
				  good results w.r.t SAS-SF and also w.r.t. the sequential algorithm. But not
				  fully scalability
	  		\item SAS-B termination condition is not well-performing on large graphs. This 
					happens because a thread has no way to shortcut the barrier if it's receiving work from
					another thread but it has to wait until all the threads have reached the barrier.
	  		\item SAS-SF behaves overall better. Performances are always
					better compared to the SAS-B algorithm and this is confirmed as the most well-performing
					distributed termination condition.
		\end{itemize}
	\end{frame}
	\begin{frame}{\secname : \subsecname}
		\framesubtitle{Comments and overall results}
		HDA* is not performing really well on our problem. Some reasons could be:
		\begin{itemize}
			\item The hash function has an important impact on the performances of A* algorithm. The one that
			we have used is probably not the optimal.
			\item The termination condition is clearly a bottleneck, in particular if the Barrier method
					is used.
			\item Resource contention is another important element. We have proved by using MP-SM that when the number of shared resources
					is minimum and thus the need of mutual exclusion access to data structures is almost absent threads work
					better (sometimes achieving better results w.r.t the sequential algorithm). On the other hand this forces
					threads to use communication method like Shared Memory that can be quite resource expensive and not fully
					scalable.
		  \end{itemize}
	\end{frame}
	%% %%%%%%%%%%%%%%%%%% %%
	%% CHAPTER 7: PNBA A* %%
	%% %%%%%%%%%%%%%%%%%% %%
	\subsection{PNBA*}
	\subsubsection{NBA*}
	\begin{frame}{\secname : \subsecname}
		\framesubtitle{NBA*}
		The NBA* algorithm is a version of  the bidirectional search that uses a data
		structure $M$ to keep track of the nodes in the middle between the two searcher
		threads $t_G$ and $t_R$. $M$ initially contains all the nodes of the graph. The nodes
		in the search frontiers are the ones that:
		\begin{itemize}
		\item Belongs to $M$
		\item Have been labelled: $g_G(n) < \infty $ or $g_R(n) < \infty $
		\end{itemize}
		The threads $t_G$ and $t_R$ share a variable $L$ initialized to $\infty$ that contains
		the cost of the best path from $source$ to $dest$. Other common variables are:
		\begin{itemize}
		\item $F_G$: lowest $f_G$ value on $t_G$ frontier.
		\item $F_R$: lowest $f_R$ value on $t_R$ frontier.
		\item Variables $F_p$, $f_p$, $g_p$ (with $p \in \{R,G\}$) are written on only one
				side but read by both sides.
		\end{itemize}
	\end{frame}
	\begin{frame}{\secname : \subsecname}
		\framesubtitle{NBA*}
		These are the initialization steps done by $t_G$ (same for $t_R$)
		\begin{itemize}
		\item $g_G(source_G)=0$, $F_G(source_G)=f_G(source_G)$
		\end{itemize}
		At each iteration it is extracted a node $x$ such that:
		\begin{itemize}
		\item $x \in M$
		\item $x: f_G(x) = min f_G(v) \forall v \in openSet_G$ 
		\end{itemize}
		The node is removed from $M$ and pruned (not expanded) if $f_G(x) \ge L$ or
		$g_G(x)+F_R-h_R(x) \ge L$. Otherwise all its successors $y$ are generated. In the
		first case it is classified as \textit{rejected} while in the other situation it is
		\textit{stabilized} because $g_G(x)$ won't be changed anymore. For each $y$ we update:
		\begin{itemize}
		\item $g_G(x)$: $min(g_G(y), g_G(x) + d_G(x, y))$
		\item $L$: $min(L, g_G(x) + g_G(y))$
		\end{itemize}
		The algorithm stops when no more candidates have to be expanded in one of the two sides.
	\end{frame}
	\subsubsection{PNBA*}
	\begin{frame}{\secname : \subsecname}
		\framesubtitle{PNBA*}
		The PNBA* algorithm improves the NBA* algorithm by letting the two threads working in
		parallel (and not in an alternate mode). This requires to cope with mutual exclusion on some data.
		\begin{figure}[ht!]
			\centering
			\includegraphics[width=0.7\linewidth]{pnba/pnbamap.png}
			\caption{PNBA* work on BAY(left) and FLA(right)}
			\label{pnbamap}
		\end{figure}
	\end{frame}
	\subsubsection{PNBA* pseudocode}
	\begin{frame}[allowframebreaks]{\secname}
		\framesubtitle{\subsecname}
		\begin{algorithmic}[1]
			\Function{$pnba_G()$}{}
				\While{$!finished$}
					\State $x \gets openSet_G.POP()$\; 
					\If{$x \in M$} 
							\If{$(f_G(x) < L) \wedge (g_G(x) + F_R - h_R(x) < L$)}
								\For{all edges $(x, y)$}
									\If{$(y \in M) \wedge (g_G(x) > g_G(x) + d_G(x, y)$}
										\State $g_G(y) \gets g_G(x) + d_G(x, y)$
										\State $f_G(y) \gets g_G(x) + h_G(x, y)$
										\If{$y \in openSet_G$}
											\State $openSet_G.REMOVE(y)$
										\EndIf
										\State $openSet_G.PUSH(\{f_G(y), y\})$
										\If{$g_G(y) + g_R(y) < L$}
											\State $lock$\;
											\If{$g_G(y) + g_R(y) < L$}
												\State $L \gets g_G(y) + g_R(y)$\;
											\EndIf
											\State $unlock$\;
										\EndIf
									\EndIf
								\EndFor
							\EndIf
							\State $ M \gets M - \{x\}$\;
					\EndIf
					\If{$!openSet_G.EMPTY()$}
						\State $F_G \gets f[openSet_G.PEEK()]$\;
						\Else
						 	\State $finished \gets true$\;
					\EndIf
				\EndWhile
			\EndFunction
		  \end{algorithmic}
	\end{frame}
	\begin{frame}{\secname : \subsecname}
		\framesubtitle{Results}
		\begin{itemize}
			\item The PNBA* is able to outperform the sequential algorithm in terms of execution time in all the 
				  graphs we have tested it on.
			\item The speed-up increases as the number of nodes increases and this can 
				  be a good news if we will try to implement it on much bigger graps.
			\item The execution time and the number of
				  nodes have been proved to strongly depend on the position of the common node found. Best performances
				  are achieved when the common node found is approximately in between of source and destination nodes.
			\item Resource consumption is almost 2x w.r.t. the sequential algorithm and this is reasonable considering
				  that it is like running two sequential algorithm in concurrency
		\end{itemize}
	\end{frame}
	\begin{frame}{\secname : \subsecname}
		\framesubtitle{Results}
		\begin{table}[ht!]
			\centering
			\caption{PNBA* - time performances}
			\begin{tabular}{|l|l|l|l|}
			\hline
			\textbf{}    & \textbf{PNBA*} & \textbf{Sequential A*} & \textbf{Speed-Up} \\ \hline
			\textbf{BAY} & 0.2091s        & 0.2197s                & 4.82\%            \\ \hline
			\textbf{FLA} & 0.6782s        & 0.7174s                & 5.46\%            \\ \hline
			\textbf{W}   & 2.3029s        & 2.5890s                & 11.1\%            \\ \hline
			\textbf{USA} & 9.0568         & 13.6716s               & 33.8\%            \\ \hline
			\end{tabular}
		  \end{table}
	\end{frame}
	\begin{frame}{\secname : \subsecname}
		\framesubtitle{Results}
		\begin{figure}[ht!]
			\centering
			\includegraphics[width=0.85\linewidth]{pnba/pnba_time.png}
			\caption{PNBA* overall performances}
		  \end{figure}
	\end{frame}
	\begin{frame}{\secname : \subsecname}
		\framesubtitle{Results}
		\begin{figure}[ht!]
			\centering
			\includegraphics[width=0.85\linewidth]{pnba/pnba_cpumem.png}
			\caption{PNBA* overall performances - time}
		  \end{figure}
	\end{frame}
	%% %%%%%%%%%%%%%%%%%%%%%% %%
	%% CHAPTER 8: CONCLUSIONS %%
	%% %%%%%%%%%%%%%%%%%%%%%% %%
	\section{Conclusions}
	\begin{frame}{\secname}
		\begin{figure}[ht!]
			\centering
			\includegraphics[width=0.85\linewidth]{others/all_algo_time.png}
			\caption{Parallel A* overall performances - time}
		\end{figure}
	\end{frame}
	\begin{frame}{\secname}
		\begin{figure}[ht!]
			\centering
			\includegraphics[width=0.85\linewidth]{others/all_algo_time_log.png}
			\caption{Parallel A* overall performances - time (log. scale)}
		\end{figure}
	\end{frame}
	\begin{frame}{\secname}
		\begin{figure}[ht!]
			\centering
			\includegraphics[width=0.85\linewidth]{others/all_algo_cpumem.png}
			\caption{Parallel A* overall performances - resources}
		\end{figure}
	\end{frame}
	\begin{frame}{\secname}
		\begin{itemize}
			\item HDA* with SAS-SF is the one that achieves worst results in terms of execution time and CPU usage.
			\item HDA* with MP-SM (using SF termination condition) is the one that behaves better in terms of execution
				  time both on BAy and FLA but it's not fully scalable. Moreover good results are achieved in terms of
				  CPU because of the limited resource contention among threads.
			\item PNBA* is the more scalable one and the only one that was proved to achive better results in terms of
				  execution time on all the benchmark maps we have used. Also reasonable performances are achived in terms
				  of CPU and memory usage.
		  \end{itemize}
	\end{frame}
	%% %%%%%%%%%%%%%%%%%%%%%% %%
	%% CHAPTER 11: REFERENCES %%
	%% %%%%%%%%%%%%%%%%%%%%%% %%
	\section{References}
	\begin{frame}[allowframebreaks]{\secname}
		\begin{thebibliography}{9}
			\bibitem{Enia} A parallel bidirectional heuristic search algorithm. (n.d.). Retrieved September 7, 2022, 
			from \url{https://homepages.dcc.ufmg.br/~chaimo/public/ENIA11.pdf}
			\bibitem{mit} Parallel A* graph search - massachusetts institute of technology. (n.d.). Retrieved September 7, 2022, from 
			\url{https://people.csail.mit.edu/rholladay/docs/parallel_search_report.pdf}
			\bibitem{stanford}Heuristics. (n.d.). Retrieved September 7, 2022, from 
			\url{http://theory.stanford.edu/~amitp/GameProgramming/Heuristics.html}
			\bibitem{wikipedia}Wikimedia Foundation. (2022, August 18). A* search algorithm. Wikipedia. Retrieved September 7, 2022, 
			from \url{https://en.wikipedia.org/wiki/A*_search_algorithm}
			\bibitem{ijcai}Proceedings of the twenty-second international joint conference - IJCAI. (n.d.). Retrieved September 7, 2022, 
			from \url{https://www.ijcai.org/Proceedings/11/Papers/105.pdf}
		  \end{thebibliography}
	\end{frame}
\end{document}